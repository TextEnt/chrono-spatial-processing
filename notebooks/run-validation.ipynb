{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run validation of LLM-judge evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aisuite as ai\n",
    "from pathlib import Path\n",
    "from textentlib.utils import read_configuration\n",
    "from textentlib.llm_utils import fetch_prompts\n",
    "from textentlib.llm_utils import query_llm, serialize_llm_responses, LLMrequest, LLMresponse, gt_metadata_to_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_configuration(Path('../data/config.yaml'))\n",
    "llms = config['validation']['models']\n",
    "#base_path = Path('/Users/mromanel/Documents/UniGe-TextEnt/chrono-spatial-processing/')\n",
    "base_path = Path('/home/users/r/romanelm/chrono-spatial-processing')\n",
    "gt_path = base_path / config['validation']['groundtruth_path']\n",
    "pregen_prompts_path = base_path / config['validation']['pregenerated_prompts_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ollama:phi4-mini:latest\n",
      "ollama:gemma3:12b\n",
      "ollama:mistral-small:24b\n",
      "ollama:deepseek-r1:14b\n",
      "ollama:deepseek-r1:32b\n",
      "openai:o1-mini\n",
      "openai:gpt-4o\n",
      "deepseek:deepseek-reasoner\n",
      "anthropic:claude-3-7-sonnet-20250219\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(llms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get validation docs and related prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gt_metadata = gt_metadata_to_dataframe(Path(gt_path))\n",
    "columns_to_keep = ['author', 'title', 'publication_date', 'document_length', 'keep_fine_tuning']\n",
    "df_annotated_docs = df_gt_metadata[(df_gt_metadata.exclude == 0) & (df_gt_metadata.annotated == 1)][columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation_docs = df_annotated_docs[df_annotated_docs.keep_fine_tuning == 1]\n",
    "validation_doc_ids = df_validation_docs.index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bpt6k10901623',\n",
       " 'bpt6k9807756q',\n",
       " 'bpt6k852913n',\n",
       " 'bpt6k5772699f',\n",
       " 'bpt6k1090242p']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_requests = fetch_prompts(pregen_prompts_path, validation_doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(llm_requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping request for document bpt6k10901623[prompt-summary.txt] using model ollama:phi4-mini:latest as it already exists\n",
      "Skipping request for document bpt6k10901623[prompt-excerpt.txt] using model ollama:phi4-mini:latest as it already exists\n",
      "Skipping request for document bpt6k10901623[prompt-metadata.txt] using model ollama:phi4-mini:latest as it already exists\n",
      "Skipping request for document bpt6k9807756q[prompt-summary.txt] using model ollama:phi4-mini:latest as it already exists\n",
      "Skipping request for document bpt6k9807756q[prompt-excerpt.txt] using model ollama:phi4-mini:latest as it already exists\n",
      "Skipping request for document bpt6k9807756q[prompt-metadata.txt] using model ollama:phi4-mini:latest as it already exists\n",
      "Skipping request for document bpt6k5772699f[prompt-excerpt.txt] using model ollama:phi4-mini:latest as it already exists\n",
      "Skipping request for document bpt6k5772699f[prompt-summary.txt] using model ollama:phi4-mini:latest as it already exists\n",
      "Skipping request for document bpt6k5772699f[prompt-metadata.txt] using model ollama:phi4-mini:latest as it already exists\n",
      "Skipping request for document bpt6k1090242p[prompt-summary.txt] using model ollama:phi4-mini:latest as it already exists\n",
      "Skipping request for document bpt6k1090242p[prompt-excerpt.txt] using model ollama:phi4-mini:latest as it already exists\n",
      "Skipping request for document bpt6k1090242p[prompt-metadata.txt] using model ollama:phi4-mini:latest as it already exists\n",
      "Skipping request for document bpt6k852913n[prompt-metadata.txt] using model ollama:phi4-mini:latest as it already exists\n",
      "Skipping request for document bpt6k852913n[prompt-excerpt.txt] using model ollama:phi4-mini:latest as it already exists\n",
      "Skipping request for document bpt6k852913n[prompt-summary.txt] using model ollama:phi4-mini:latest as it already exists\n",
      "Processing prompt prompt-summary.txt for document bpt6k10901623 using model ollama:gemma3:12b\n",
      "Time taken to get response: 24.94 seconds. Total tokens: None\n",
      "Processing prompt prompt-excerpt.txt for document bpt6k10901623 using model ollama:gemma3:12b\n",
      "Time taken to get response: 6.87 seconds. Total tokens: None\n",
      "Processing prompt prompt-metadata.txt for document bpt6k10901623 using model ollama:gemma3:12b\n",
      "Time taken to get response: 7.76 seconds. Total tokens: None\n",
      "Processing prompt prompt-summary.txt for document bpt6k9807756q using model ollama:gemma3:12b\n",
      "Time taken to get response: 10.34 seconds. Total tokens: None\n",
      "Processing prompt prompt-excerpt.txt for document bpt6k9807756q using model ollama:gemma3:12b\n",
      "Time taken to get response: 6.44 seconds. Total tokens: None\n",
      "Processing prompt prompt-metadata.txt for document bpt6k9807756q using model ollama:gemma3:12b\n",
      "Time taken to get response: 5.53 seconds. Total tokens: None\n",
      "Processing prompt prompt-excerpt.txt for document bpt6k5772699f using model ollama:gemma3:12b\n",
      "Time taken to get response: 7.38 seconds. Total tokens: None\n",
      "Processing prompt prompt-summary.txt for document bpt6k5772699f using model ollama:gemma3:12b\n",
      "Time taken to get response: 9.22 seconds. Total tokens: None\n",
      "Processing prompt prompt-metadata.txt for document bpt6k5772699f using model ollama:gemma3:12b\n",
      "Time taken to get response: 7.17 seconds. Total tokens: None\n",
      "Processing prompt prompt-summary.txt for document bpt6k1090242p using model ollama:gemma3:12b\n",
      "Time taken to get response: 9.90 seconds. Total tokens: None\n",
      "Processing prompt prompt-excerpt.txt for document bpt6k1090242p using model ollama:gemma3:12b\n",
      "Time taken to get response: 8.72 seconds. Total tokens: None\n",
      "Processing prompt prompt-metadata.txt for document bpt6k1090242p using model ollama:gemma3:12b\n",
      "Time taken to get response: 6.18 seconds. Total tokens: None\n",
      "Processing prompt prompt-metadata.txt for document bpt6k852913n using model ollama:gemma3:12b\n",
      "Time taken to get response: 6.07 seconds. Total tokens: None\n",
      "Processing prompt prompt-excerpt.txt for document bpt6k852913n using model ollama:gemma3:12b\n",
      "Time taken to get response: 7.87 seconds. Total tokens: None\n",
      "Processing prompt prompt-summary.txt for document bpt6k852913n using model ollama:gemma3:12b\n",
      "Time taken to get response: 8.67 seconds. Total tokens: None\n",
      "Processing prompt prompt-summary.txt for document bpt6k10901623 using model ollama:mistral-small:24b\n",
      "Time taken to get response: 39.68 seconds. Total tokens: None\n",
      "Processing prompt prompt-excerpt.txt for document bpt6k10901623 using model ollama:mistral-small:24b\n",
      "Time taken to get response: 11.12 seconds. Total tokens: None\n",
      "Processing prompt prompt-metadata.txt for document bpt6k10901623 using model ollama:mistral-small:24b\n",
      "Time taken to get response: 7.98 seconds. Total tokens: None\n",
      "Processing prompt prompt-summary.txt for document bpt6k9807756q using model ollama:mistral-small:24b\n",
      "Time taken to get response: 12.23 seconds. Total tokens: None\n",
      "Processing prompt prompt-excerpt.txt for document bpt6k9807756q using model ollama:mistral-small:24b\n",
      "Time taken to get response: 10.79 seconds. Total tokens: None\n",
      "Processing prompt prompt-metadata.txt for document bpt6k9807756q using model ollama:mistral-small:24b\n",
      "Time taken to get response: 9.76 seconds. Total tokens: None\n",
      "Processing prompt prompt-excerpt.txt for document bpt6k5772699f using model ollama:mistral-small:24b\n",
      "Time taken to get response: 10.76 seconds. Total tokens: None\n",
      "Processing prompt prompt-summary.txt for document bpt6k5772699f using model ollama:mistral-small:24b\n",
      "Time taken to get response: 13.49 seconds. Total tokens: None\n",
      "Processing prompt prompt-metadata.txt for document bpt6k5772699f using model ollama:mistral-small:24b\n",
      "Time taken to get response: 9.97 seconds. Total tokens: None\n",
      "Processing prompt prompt-summary.txt for document bpt6k1090242p using model ollama:mistral-small:24b\n",
      "Time taken to get response: 13.21 seconds. Total tokens: None\n",
      "Processing prompt prompt-excerpt.txt for document bpt6k1090242p using model ollama:mistral-small:24b\n",
      "Time taken to get response: 11.57 seconds. Total tokens: None\n",
      "Processing prompt prompt-metadata.txt for document bpt6k1090242p using model ollama:mistral-small:24b\n",
      "Time taken to get response: 10.82 seconds. Total tokens: None\n",
      "Processing prompt prompt-metadata.txt for document bpt6k852913n using model ollama:mistral-small:24b\n",
      "Time taken to get response: 8.30 seconds. Total tokens: None\n",
      "Processing prompt prompt-excerpt.txt for document bpt6k852913n using model ollama:mistral-small:24b\n",
      "Time taken to get response: 9.83 seconds. Total tokens: None\n",
      "Processing prompt prompt-summary.txt for document bpt6k852913n using model ollama:mistral-small:24b\n",
      "Time taken to get response: 12.84 seconds. Total tokens: None\n",
      "Processing prompt prompt-summary.txt for document bpt6k10901623 using model ollama:deepseek-r1:14b\n",
      "Time taken to get response: 42.29 seconds. Total tokens: None\n",
      "Processing prompt prompt-excerpt.txt for document bpt6k10901623 using model ollama:deepseek-r1:14b\n",
      "Time taken to get response: 35.47 seconds. Total tokens: None\n",
      "Processing prompt prompt-metadata.txt for document bpt6k10901623 using model ollama:deepseek-r1:14b\n",
      "Time taken to get response: 20.77 seconds. Total tokens: None\n",
      "Processing prompt prompt-summary.txt for document bpt6k9807756q using model ollama:deepseek-r1:14b\n",
      "Time taken to get response: 36.74 seconds. Total tokens: None\n",
      "Processing prompt prompt-excerpt.txt for document bpt6k9807756q using model ollama:deepseek-r1:14b\n",
      "Time taken to get response: 19.39 seconds. Total tokens: None\n",
      "Processing prompt prompt-metadata.txt for document bpt6k9807756q using model ollama:deepseek-r1:14b\n",
      "Time taken to get response: 25.61 seconds. Total tokens: None\n",
      "Processing prompt prompt-excerpt.txt for document bpt6k5772699f using model ollama:deepseek-r1:14b\n",
      "Time taken to get response: 22.04 seconds. Total tokens: None\n",
      "Processing prompt prompt-summary.txt for document bpt6k5772699f using model ollama:deepseek-r1:14b\n",
      "Time taken to get response: 30.60 seconds. Total tokens: None\n",
      "Processing prompt prompt-metadata.txt for document bpt6k5772699f using model ollama:deepseek-r1:14b\n",
      "Time taken to get response: 22.63 seconds. Total tokens: None\n",
      "Processing prompt prompt-summary.txt for document bpt6k1090242p using model ollama:deepseek-r1:14b\n",
      "Time taken to get response: 35.78 seconds. Total tokens: None\n",
      "Processing prompt prompt-excerpt.txt for document bpt6k1090242p using model ollama:deepseek-r1:14b\n",
      "Time taken to get response: 22.36 seconds. Total tokens: None\n",
      "Processing prompt prompt-metadata.txt for document bpt6k1090242p using model ollama:deepseek-r1:14b\n",
      "Time taken to get response: 19.13 seconds. Total tokens: None\n",
      "Processing prompt prompt-metadata.txt for document bpt6k852913n using model ollama:deepseek-r1:14b\n",
      "Time taken to get response: 28.39 seconds. Total tokens: None\n",
      "Processing prompt prompt-excerpt.txt for document bpt6k852913n using model ollama:deepseek-r1:14b\n",
      "Time taken to get response: 26.44 seconds. Total tokens: None\n",
      "Processing prompt prompt-summary.txt for document bpt6k852913n using model ollama:deepseek-r1:14b\n",
      "Time taken to get response: 40.19 seconds. Total tokens: None\n",
      "Processing prompt prompt-summary.txt for document bpt6k10901623 using model ollama:deepseek-r1:32b\n",
      "Time taken to get response: 100.45 seconds. Total tokens: None\n",
      "Processing prompt prompt-excerpt.txt for document bpt6k10901623 using model ollama:deepseek-r1:32b\n",
      "Time taken to get response: 56.63 seconds. Total tokens: None\n",
      "Processing prompt prompt-metadata.txt for document bpt6k10901623 using model ollama:deepseek-r1:32b\n",
      "Time taken to get response: 66.59 seconds. Total tokens: None\n",
      "Processing prompt prompt-summary.txt for document bpt6k9807756q using model ollama:deepseek-r1:32b\n",
      "Time taken to get response: 52.18 seconds. Total tokens: None\n",
      "Processing prompt prompt-excerpt.txt for document bpt6k9807756q using model ollama:deepseek-r1:32b\n",
      "Time taken to get response: 35.57 seconds. Total tokens: None\n",
      "Processing prompt prompt-metadata.txt for document bpt6k9807756q using model ollama:deepseek-r1:32b\n",
      "Time taken to get response: 38.79 seconds. Total tokens: None\n",
      "Processing prompt prompt-excerpt.txt for document bpt6k5772699f using model ollama:deepseek-r1:32b\n",
      "Time taken to get response: 39.39 seconds. Total tokens: None\n",
      "Processing prompt prompt-summary.txt for document bpt6k5772699f using model ollama:deepseek-r1:32b\n",
      "Time taken to get response: 43.53 seconds. Total tokens: None\n",
      "Processing prompt prompt-metadata.txt for document bpt6k5772699f using model ollama:deepseek-r1:32b\n",
      "Time taken to get response: 30.61 seconds. Total tokens: None\n",
      "Processing prompt prompt-summary.txt for document bpt6k1090242p using model ollama:deepseek-r1:32b\n",
      "Time taken to get response: 54.74 seconds. Total tokens: None\n",
      "Processing prompt prompt-excerpt.txt for document bpt6k1090242p using model ollama:deepseek-r1:32b\n",
      "Time taken to get response: 47.13 seconds. Total tokens: None\n",
      "Processing prompt prompt-metadata.txt for document bpt6k1090242p using model ollama:deepseek-r1:32b\n",
      "Time taken to get response: 48.91 seconds. Total tokens: None\n",
      "Processing prompt prompt-metadata.txt for document bpt6k852913n using model ollama:deepseek-r1:32b\n",
      "Time taken to get response: 34.30 seconds. Total tokens: None\n",
      "Processing prompt prompt-excerpt.txt for document bpt6k852913n using model ollama:deepseek-r1:32b\n",
      "Time taken to get response: 61.21 seconds. Total tokens: None\n",
      "Processing prompt prompt-summary.txt for document bpt6k852913n using model ollama:deepseek-r1:32b\n",
      "Time taken to get response: 47.43 seconds. Total tokens: None\n",
      "Processing prompt prompt-summary.txt for document bpt6k10901623 using model openai:o1-mini\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "OpenAI API key is missing. Please provide it in the config or set the OPENAI_API_KEY environment variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m client\u001b[38;5;241m.\u001b[39mconfigure({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mollama\u001b[39m\u001b[38;5;124m\"\u001b[39m : {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m600\u001b[39m}})\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m llms:\n\u001b[0;32m----> 8\u001b[0m     llm_responses \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mquery_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_requests\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_responses_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/chrono-spatial-processing/textentlib/llm_utils.py:79\u001b[0m, in \u001b[0;36mquery_llm\u001b[0;34m(client, model, requests, output_path)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Capture the start time\u001b[39;00m\n\u001b[1;32m     78\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 79\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Capture the end time\u001b[39;00m\n\u001b[1;32m     81\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/aisuite/client.py:108\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, model, messages, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m provider_key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mproviders:\n\u001b[1;32m    107\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mprovider_configs\u001b[38;5;241m.\u001b[39mget(provider_key, {})\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mproviders[provider_key] \u001b[38;5;241m=\u001b[39m \u001b[43mProviderFactory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_provider\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprovider_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m provider \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mproviders\u001b[38;5;241m.\u001b[39mget(provider_key)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m provider:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/aisuite/provider.py:46\u001b[0m, in \u001b[0;36mProviderFactory.create_provider\u001b[0;34m(cls, provider_key, config)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Instantiate the provider class\u001b[39;00m\n\u001b[1;32m     45\u001b[0m provider_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, provider_class_name)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprovider_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/aisuite/providers/openai_provider.py:15\u001b[0m, in \u001b[0;36mOpenaiProvider.__init__\u001b[0;34m(self, **config)\u001b[0m\n\u001b[1;32m     13\u001b[0m config\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_key\u001b[39m\u001b[38;5;124m\"\u001b[39m, os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_key\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAI API key is missing. Please provide it in the config or set the OPENAI_API_KEY environment variable.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m     )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# NOTE: We could choose to remove above lines for api_key since OpenAI will automatically\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# infer certain values from the environment variables.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Eg: OPENAI_API_KEY, OPENAI_ORG_ID, OPENAI_PROJECT_ID, OPENAI_BASE_URL, etc.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Pass the entire config to the OpenAI client constructor\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mOpenAI(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n",
      "\u001b[0;31mValueError\u001b[0m: OpenAI API key is missing. Please provide it in the config or set the OPENAI_API_KEY environment variable."
     ]
    }
   ],
   "source": [
    "llm_responses = []\n",
    "llm_responses_path = Path('../data/validation/llm_responses')\n",
    "\n",
    "client = ai.Client()\n",
    "client.configure({\"ollama\" : {\"timeout\": 600}})\n",
    "\n",
    "for model in llms:\n",
    "    llm_responses += query_llm(client, model, llm_requests, llm_responses_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for human scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
