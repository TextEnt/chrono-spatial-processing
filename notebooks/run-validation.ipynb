{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run validation of LLM-judge evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "import aisuite as ai\n",
    "from pathlib import Path\n",
    "from textentlib.utils import read_configuration\n",
    "from textentlib.llm_utils import fetch_prompts, try_extract_json_from_text\n",
    "from textentlib.llm_utils import prepare_evaluation_dataframe, query_llm, query_llm_judge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_configuration(Path('../data/config.yaml'))\n",
    "llms = config['validation']['models']\n",
    "base_path = Path('/Users/mromanel/Documents/UniGe-TextEnt/chrono-spatial-processing/')\n",
    "gt_path = base_path / config['validation']['groundtruth_path']\n",
    "pregen_prompts_path = base_path / config['validation']['pregenerated_prompts_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ollama:phi4-mini:latest\n",
      "ollama:gemma3:12b\n",
      "ollama:mistral-small:24b\n",
      "ollama:deepseek-r1:14b\n",
      "ollama:deepseek-r1:32b\n",
      "openai:o1-mini\n",
      "openai:gpt-4o\n",
      "deepseek:deepseek-reasoner\n",
      "anthropic:claude-3-7-sonnet-20250219\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(llms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get LLM predictions on validation documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_llm_responses_path = Path(base_path / config['validation']['responses_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"['Unnamed: 11', 'Unnamed: 12'] not found in axis\"\n"
     ]
    }
   ],
   "source": [
    "# this should return only the test (validation) set (n=5 docs)\n",
    "validation_docs, df_validation_data = prepare_evaluation_dataframe(\n",
    "    llm_response_path=validation_llm_responses_path,\n",
    "    gt_annotations_path=gt_path,\n",
    "    gt_metadata_path=gt_path,\n",
    "    split=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation_data.to_csv('../data/evaluation_data.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_docs = ['bpt6k10901623',\n",
    " 'bpt6k9807756q',\n",
    " 'bpt6k852913n',\n",
    " 'bpt6k5772699f',\n",
    " 'bpt6k1090242p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_requests = fetch_prompts(Path(pregen_prompts_path), validation_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(llm_requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LLMrequest(prompt_id='prompt-excerpt.txt', document_id='bpt6k10901623', prompt_path=PosixPath('/Users/mromanel/Documents/UniGe-TextEnt/chrono-spatial-processing/data/prompts/pregenerated/bpt6k10901623/bpt6k10901623_prompt-excerpt.txt'), prompt='Look at the following JSON object describing a theatre play in French (XVII century); the `metadata` property contains basic information about the play (author, title, publication date), while the `excerpt` property contains an excerpt of 400 words sampled from around the middle of the document.\\n\\nINPUT:\\n```json\\n{\\n  \"metadata\": {\\n    \"author\": \"Boisrobert, François de\",\\n    \"title\": \"Théodore, Reyne de Hongrie, tragi-comédie\",\\n    \"publication_date\": \"1658\",\\n    \"document_id\": \"bpt6k10901623\"\\n  },\\n  \"excerpt\": \"re; Oui j\\'ai pitié de vous, Prince, et je vous promets, Si vous vous repentez, de n\\'y penser jamais, Je me reprocherai cette ardeur enragée, Comme si on l\\'avais bizarrement songée, Revenez donc à vous, ouvrez, ouurez les yeux, Et voyez où vous porte un désir furieux, Seriez-vous pas perdu si le Roi qui vous aime, Pouvait être averti de ce désordre extrême, Savez-vous qui ie suis, me connaissez-vou\"\\n}\\n```\\n\\nYour role is to predict the location and historical period in which the action of the play is set. \\n\\nKEY RULES:\\n- Predict the timespan and not the precise and exact date of the period where the play could have taken place\\n- Do not write an introduction or summary \\n- The response must contain only valid JSON\\n- The values in the JSON \"timeframe_start\" and \"timeframe_end\" should always be a single valid date in the form [±Y]YYYY; negative values should be used for years before common era B.C.E. (e.g. `300 B.C.` should be represented as `-300`)\\n- if the provided information is not sufficient to determine historical period and/or location, the following values can be set to `None`: `period`, `timeframe_start`, `timeframe_end`, `location`, `location_qid`\\n\\nReturn your response and the underlying reasoning as a JSON object with the following structure:\\n```json\\n{\\n    \"period\": \"The historical period in which the play could have taken place\",\\n    \"period_reasoning\": \"The reasoning the model used to identify the historical period\",\\n    \"timeframe_start\": \"The start value of the historical period, formatted as [±Y]YYYY\",\\n    \"timeframe_end\": \"The end value of the historical period, formatted as [±Y]YYYY\",\\n    \"location\": \"The geographic location where the action of the play takes place\",\\n    \"location_reasoning\": \"The reasoning the model used to identify the geographic location\",\\n    \"location_qid\": \"The Wikidata QID of the identified location\"\\n}\\n```'),\n",
       " LLMrequest(prompt_id='prompt-metadata.txt', document_id='bpt6k10901623', prompt_path=PosixPath('/Users/mromanel/Documents/UniGe-TextEnt/chrono-spatial-processing/data/prompts/pregenerated/bpt6k10901623/bpt6k10901623_prompt-metadata.txt'), prompt='Look at the following JSON object describing a theatre play in French (XVII century); the `metadata` property contains basic information about the play (author, title, publication date).\\n\\nINPUT:\\n```json\\n{\\n  \"metadata\": {\\n    \"author\": \"Boisrobert, François de\",\\n    \"title\": \"Théodore, Reyne de Hongrie, tragi-comédie\",\\n    \"publication_date\": \"1658\",\\n    \"document_id\": \"bpt6k10901623\"\\n  }\\n}\\n```\\n\\nYour role is to predict the location and historical period in which the action of the play is set. \\n\\nKEY RULES:\\n- Predict the timespan and not the precise and exact date of the period where the play could have taken place\\n- Do not write an introduction or summary \\n- The response must contain only valid JSON\\n- The values in the JSON \"timeframe_start\" and \"timeframe_end\" should always be a single valid date in the form [±Y]YYYY; negative values should be used for years before common era B.C.E. (e.g. `300 B.C.` should be represented as `-300`)\\n- if the provided information is not sufficient to determine historical period and/or location, the following values can be set to `None`: `period`, `timeframe_start`, `timeframe_end`, `location`, `location_qid`\\n\\nReturn your response and the underlying reasoning as a JSON object with the following structure:\\n```json\\n{\\n    \"period\": \"The historical period in which the play could have taken place\",\\n    \"period_reasoning\": \"The reasoning the model used to identify the historical period\",\\n    \"timeframe_start\": \"The start value of the historical period, formatted as [±Y]YYYY\",\\n    \"timeframe_end\": \"The end value of the historical period, formatted as [±Y]YYYY\",\\n    \"location\": \"The geographic location where the action of the play takes place\",\\n    \"location_reasoning\": \"The reasoning the model used to identify the geographic location\",\\n    \"location_qid\": \"The Wikidata QID of the identified location\"\\n}\\n```'),\n",
       " LLMrequest(prompt_id='prompt-summary.txt', document_id='bpt6k10901623', prompt_path=PosixPath('/Users/mromanel/Documents/UniGe-TextEnt/chrono-spatial-processing/data/prompts/pregenerated/bpt6k10901623/bpt6k10901623_prompt-summary.txt'), prompt='Look at the following JSON object describing a theatre play in French (XVII century); the `metadata` property contains basic information about the play (author, title, publication date), while the `context` property contains information about the people and places that are most frequently mentioned in the play (such as label, mention frequency, and salient sentences where it appears).\\n\\nINPUT:\\n```json\\n{\\n  \"metadata\": {\\n    \"author\": \"Boisrobert, François de\",\\n    \"title\": \"Théodore, Reyne de Hongrie, tragi-comédie\",\\n    \"publication_date\": \"1658\",\\n    \"document_id\": \"bpt6k10901623\"\\n  },\\n  \"context\": {\\n    \"people\": {\\n      \"top_1_person\": {\\n        \"entity\": {\\n          \"label\": \"RAMÉE\",\\n          \"frequency\": 1\\n        },\\n        \"related_sentences\": [\\n          \"Me voyant emporté tu pouvais m\\'écouter, Mais tu ne devais pas si vite exécuter, colère aueugle, aueuglement suiuie, Me va coûter honneur, le repos et la vie, serviteur fidèle agit plus prudemment, \\\\nRAMÉE\\\\n Vous voulez qu\\'on vous serve en tout aveuglément : Je voyais bien, Seigneur, que votre ordre était Jude, Mais tu craignais l\\'effet de votre promptitude, J\\'en ai vu dans la Cour l\\'exemple dangereux, J\\'ai vu périr des Grands, tu me règle par eux, LE ROY.\",\\n          \"LE ROY Cet ordre est surprenant, il est grand, il t\\'étonne, Mais, Ramèse, en un mot il faut l\\'exécuter, De ta fidélité Seigneur ne saurais douter, Va, lcrime est connu, \\'ai tout su de mon frère, La Reylie a dals son tœur commis un Adultère, Elle est digne de mort, va donc de ce pas, Exécuter mon ordre et ne réplique pas. \\\\nRAMÉE\\\\n J\\'obéirai Seigneur, LE ROY.\",\\n          \"Puis que ta main trop prompte a suivi ma pensée, Condamnons seulement celui qui l\\'a poussée, Tu suis un ordre injuste, et j\\'excuse ton bras, Mais, traître que je suis, tu ne m\\'excuse pas, Een le me parvome un coup n\\'étable. \\\\nRAMÉE\\\\n \",\\n          \"que m\\'apprendstu? \\\\nRAMÉE\\\\n u’on fait un double outrage à la même vertu, J\\'ai peur que ce récit, Seigneur, ne vous accable, 4 Vous voyez que ici tremble au seul nom du cou.\",\\n          \"Cette lettre est du Prince, \\\\nRAMÉE\\\\n Elle est du suborneur: Comme il a vainement attaqué son honneur, Elle l\\'a menacé, mais il l\\'a prévenue, Sans songer que sa lettre un jour serait connue;\"\\n        ]\\n      },\\n      \"top_5_persons\": [\\n        \"RAMÉE\",\\n        \"Irene\",\\n        \"Irene\",\\n        \"Théodore\",\\n        \"Irène\"\\n      ]\\n    },\\n    \"places\": {\\n      \"top_1_place\": {\\n        \"entity\": {\\n          \"label\": \"Asie\",\\n          \"frequency\": 1\\n        },\\n        \"related_sentences\": [\\n          \"ennemis défaits attendent du secours, Mais pour le repos de nos jours, Amurat est-pressé de passer en Asie, Je ne crains donc plus rien de l\\'armée ennemie.\"\\n        ]\\n      },\\n      \"top_5_places\": [\\n        \"Asie\",\\n        \"Moni\",\\n        \"Jap\"\\n      ]\\n    }\\n  }\\n}\\n```\\n\\nYour role is to predict the location and historical period in which the action of the play is set. \\n\\nKEY RULES:\\n- Predict the timespan and not the precise and exact date of the period where the play could have taken place\\n- Do not write an introduction or summary \\n- The response must contain only valid JSON\\n- The values in the JSON \"timeframe_start\" and \"timeframe_end\" should always be a single valid date in the form [±Y]YYYY; negative values should be used for years before common era B.C.E. (e.g. `300 B.C.` should be represented as `-300`)\\n- if the provided information is not sufficient to determine historical period and/or location, the following values can be set to `None`: `period`, `timeframe_start`, `timeframe_end`, `location`, `location_qid`\\n\\nReturn your response and the underlying reasoning as a JSON object with the following structure:\\n```json\\n{\\n    \"period\": \"The historical period in which the play could have taken place\",\\n    \"period_reasoning\": \"The reasoning the model used to identify the historical period\",\\n    \"timeframe_start\": \"The start value of the historical period, formatted as [±Y]YYYY\",\\n    \"timeframe_end\": \"The end value of the historical period, formatted as [±Y]YYYY\",\\n    \"location\": \"The geographic location where the action of the play takes place\",\\n    \"location_reasoning\": \"The reasoning the model used to identify the geographic location\",\\n    \"location_qid\": \"The Wikidata QID of the identified location\"\\n}\\n```'),\n",
       " LLMrequest(prompt_id='prompt-excerpt.txt', document_id='bpt6k1090242p', prompt_path=PosixPath('/Users/mromanel/Documents/UniGe-TextEnt/chrono-spatial-processing/data/prompts/pregenerated/bpt6k1090242p/bpt6k1090242p_prompt-excerpt.txt'), prompt='Look at the following JSON object describing a theatre play in French (XVII century); the `metadata` property contains basic information about the play (author, title, publication date), while the `excerpt` property contains an excerpt of 400 words sampled from around the middle of the document.\\n\\nINPUT:\\n```json\\n{\\n  \"metadata\": {\\n    \"author\": null,\\n    \"title\": \"Tite et Titus, ou Critique sur les Berenices, comédie\",\\n    \"publication_date\": \"1673\",\\n    \"document_id\": \"bpt6k1090242p\"\\n  },\\n  \"excerpt\": \" pas fort extraordinaist buser une fille souz promesse de mariage. On voit plus d\\'un moqueur Enée, Et plus d\\'une folle Didon Couvrer les feux de Cupidon Souz les cendres de L\\'hyménée. Voici quelque chose déplus étrange; il n\\'est rien de si touchant ainsi de si tendre que les choses qu\\'il dir il sa Berenice dans cette occalion, il semble tout à coup qu\\'il va expirer d\\'amour pour elle, enfin il est \"\\n}\\n```\\n\\nYour role is to predict the location and historical period in which the action of the play is set. \\n\\nKEY RULES:\\n- Predict the timespan and not the precise and exact date of the period where the play could have taken place\\n- Do not write an introduction or summary \\n- The response must contain only valid JSON\\n- The values in the JSON \"timeframe_start\" and \"timeframe_end\" should always be a single valid date in the form [±Y]YYYY; negative values should be used for years before common era B.C.E. (e.g. `300 B.C.` should be represented as `-300`)\\n- if the provided information is not sufficient to determine historical period and/or location, the following values can be set to `None`: `period`, `timeframe_start`, `timeframe_end`, `location`, `location_qid`\\n\\nReturn your response and the underlying reasoning as a JSON object with the following structure:\\n```json\\n{\\n    \"period\": \"The historical period in which the play could have taken place\",\\n    \"period_reasoning\": \"The reasoning the model used to identify the historical period\",\\n    \"timeframe_start\": \"The start value of the historical period, formatted as [±Y]YYYY\",\\n    \"timeframe_end\": \"The end value of the historical period, formatted as [±Y]YYYY\",\\n    \"location\": \"The geographic location where the action of the play takes place\",\\n    \"location_reasoning\": \"The reasoning the model used to identify the geographic location\",\\n    \"location_qid\": \"The Wikidata QID of the identified location\"\\n}\\n```'),\n",
       " LLMrequest(prompt_id='prompt-metadata.txt', document_id='bpt6k1090242p', prompt_path=PosixPath('/Users/mromanel/Documents/UniGe-TextEnt/chrono-spatial-processing/data/prompts/pregenerated/bpt6k1090242p/bpt6k1090242p_prompt-metadata.txt'), prompt='Look at the following JSON object describing a theatre play in French (XVII century); the `metadata` property contains basic information about the play (author, title, publication date).\\n\\nINPUT:\\n```json\\n{\\n  \"metadata\": {\\n    \"author\": null,\\n    \"title\": \"Tite et Titus, ou Critique sur les Berenices, comédie\",\\n    \"publication_date\": \"1673\",\\n    \"document_id\": \"bpt6k1090242p\"\\n  }\\n}\\n```\\n\\nYour role is to predict the location and historical period in which the action of the play is set. \\n\\nKEY RULES:\\n- Predict the timespan and not the precise and exact date of the period where the play could have taken place\\n- Do not write an introduction or summary \\n- The response must contain only valid JSON\\n- The values in the JSON \"timeframe_start\" and \"timeframe_end\" should always be a single valid date in the form [±Y]YYYY; negative values should be used for years before common era B.C.E. (e.g. `300 B.C.` should be represented as `-300`)\\n- if the provided information is not sufficient to determine historical period and/or location, the following values can be set to `None`: `period`, `timeframe_start`, `timeframe_end`, `location`, `location_qid`\\n\\nReturn your response and the underlying reasoning as a JSON object with the following structure:\\n```json\\n{\\n    \"period\": \"The historical period in which the play could have taken place\",\\n    \"period_reasoning\": \"The reasoning the model used to identify the historical period\",\\n    \"timeframe_start\": \"The start value of the historical period, formatted as [±Y]YYYY\",\\n    \"timeframe_end\": \"The end value of the historical period, formatted as [±Y]YYYY\",\\n    \"location\": \"The geographic location where the action of the play takes place\",\\n    \"location_reasoning\": \"The reasoning the model used to identify the geographic location\",\\n    \"location_qid\": \"The Wikidata QID of the identified location\"\\n}\\n```'),\n",
       " LLMrequest(prompt_id='prompt-summary.txt', document_id='bpt6k1090242p', prompt_path=PosixPath('/Users/mromanel/Documents/UniGe-TextEnt/chrono-spatial-processing/data/prompts/pregenerated/bpt6k1090242p/bpt6k1090242p_prompt-summary.txt'), prompt='Look at the following JSON object describing a theatre play in French (XVII century); the `metadata` property contains basic information about the play (author, title, publication date), while the `context` property contains information about the people and places that are most frequently mentioned in the play (such as label, mention frequency, and salient sentences where it appears).\\n\\nINPUT:\\n```json\\n{\\n  \"metadata\": {\\n    \"author\": null,\\n    \"title\": \"Tite et Titus, ou Critique sur les Berenices, comédie\",\\n    \"publication_date\": \"1673\",\\n    \"document_id\": \"bpt6k1090242p\"\\n  },\\n  \"context\": {\\n    \"people\": {\\n      \"top_1_person\": {\\n        \"entity\": {\\n          \"label\": \"THALIE\",\\n          \"frequency\": 5\\n        },\\n        \"related_sentences\": [\\n          \"Oui sans doute, savante Nim hé, et si tous les entiments de Titus m\\'ont toujours servi de Loi, ors même que j\\'ai eu sujet de les trouver les lus injustes, il n\\'y a pas apparence que je le désaoue quand il m\\'en attribue d\\'aussi raisonnables que ceux -là. \\\\nTHALIE\\\\n J\\'ai bien de la joie, illustres Amants, de vous évoir dans un si bon accord, et tous ceux, que outre séparation avait si fort affligés, Jusques à les lire fondre en Larmes, seront bien consolez, and ils sauront votre bonne intelligence prêlen: vous vous étiez pourtant séparés avec assés de eremonie, et votre à dieu avait été asséz long, our tenir plus longtemps, et pour ne vous pas revIr si tôt.\",\\n          \"Il parait bien à vos discours, savante Nim que les grâces et les jeux ne vous abandonne mais, et l\\'obligeante raillerie, dont vous u accueillie, ne pouvait être assaisonnée pa main plus délicate aussi jo la reçois avec tos reconnaissance que méritent les civilités, et louanges d\\'une immortelle comme vous. \\\\nTHALIE\\\\n Vous venez à propos en ce pays, aimable l\\'ut cesse, et le sort vous y à sans doute conduite p\",\\n          \"En effet, ma sœur n\\'a pas raison de vous cette raillerie, mais il ne faut pas que cela vousfraie: épar, outre que tout céqu\\'elle dit n\\'est lesouvent que pour rire, élle est obligée de défe vos Ennemis ayant ordre d\\'Apollon de les pie ger, comme il m\\'a chargée de vous conduire TITUS à THALIE Aimable\",\\n          \", j\\'ai encore plus grand jet que Tite d\\'appréhender qu\\'on ne me prenne our une autre, et qu\\'on ne m\\'attribue bien des hoses, qui ne me conviennent pas. \\\\nTHALIE\\\\n C\\'en est assurés pour le présent, belle Princesse ous avés apparemament plus besoin de repos que de ascours;\",\\n          \"Thalie, il ne faut pas que cette inégalité vous surprenne, la différence que vous trouvez entre notre entretien et ceux que Titus vous à rappoitéz, est une suitte nécessaire de la différence des matières, il est bien aisé d\\'être clair dans un entretien familier comme celui cy ou l\\'on ne parle que\"\\n        ]\\n      },\\n      \"top_5_persons\": [\\n        \"THALIE\",\\n        \"Tite\",\\n        \"Titus\",\\n        \"Berenice\",\\n        \"Apollon\"\\n      ]\\n    },\\n    \"places\": {\\n      \"top_1_place\": {\\n        \"entity\": {\\n          \"label\": \"Parnasse\",\\n          \"frequency\": 5\\n        },\\n        \"related_sentences\": [\\n          \"Ces noms sont en vénération au Parnasse l\\'un a té le père du Théâtre Français et l\\'autre en est le burrislier, personne, ô Melpomene, ne le sait jeux que vous, mais passons outre, vous Tite mmancéz mais acte condition de ne dire précinent que ce qui sait contre Titus et ce que vous ouvez à redire en lui etde ne dire point ce qui fait pour vous et ce qu\\'on peut vous objecter etcela re.\",\\n          \"c\\'est ainsi que Paris est devenu aujourd\\'aui le lieu du monde ou nous sommes en plus grande estime et les plus connue, aussi voyez vous qu\\'Apollon et nous ne parlons tous que Français, c\\'est jamais présent la langue du Parnasse et toute autre y est arbare, telle est la vicissitude des choses, mais j\\'apperçois, Aposson qui s\\'avance et vous allez et ugéz.\",\\n          \"c\\'est pourquoi abstenéz vous en soigneusement, si vous êtes sagé, quand vous retournerez en France, il sera permis à vous de reprendre votre jargon, puisqu\\'il y a des gens qu\\'en accommodent, mais tant que vous seréseréz au Parnasse, vivez selon la loi du pays.\",\\n          \"Ur fin qu\\'il en soit mémoire ôrmaise et pour empêcher que pareille chose ne puisse arriver à l\\'avenir, l est enioint à tous ceux qui gardent les entrées du Parnasse, qu\\'ils aient désormais ne laisser plus montrer personne de quelque qualité et condition que ce soit;\",\\n          \"soyer d les biens venus Princes, il ne vous rêle pluss nous dire de qui sont vos lettres d\\'adresse qui vous a donné passe port pour monter sur le Parnasse MELPOMENE.\"\\n        ]\\n      },\\n      \"top_5_places\": [\\n        \"Parnasse\",\\n        \"Rome\",\\n        \"France\",\\n        \"Bervie\",\\n        \"Carthage\"\\n      ]\\n    }\\n  }\\n}\\n```\\n\\nYour role is to predict the location and historical period in which the action of the play is set. \\n\\nKEY RULES:\\n- Predict the timespan and not the precise and exact date of the period where the play could have taken place\\n- Do not write an introduction or summary \\n- The response must contain only valid JSON\\n- The values in the JSON \"timeframe_start\" and \"timeframe_end\" should always be a single valid date in the form [±Y]YYYY; negative values should be used for years before common era B.C.E. (e.g. `300 B.C.` should be represented as `-300`)\\n- if the provided information is not sufficient to determine historical period and/or location, the following values can be set to `None`: `period`, `timeframe_start`, `timeframe_end`, `location`, `location_qid`\\n\\nReturn your response and the underlying reasoning as a JSON object with the following structure:\\n```json\\n{\\n    \"period\": \"The historical period in which the play could have taken place\",\\n    \"period_reasoning\": \"The reasoning the model used to identify the historical period\",\\n    \"timeframe_start\": \"The start value of the historical period, formatted as [±Y]YYYY\",\\n    \"timeframe_end\": \"The end value of the historical period, formatted as [±Y]YYYY\",\\n    \"location\": \"The geographic location where the action of the play takes place\",\\n    \"location_reasoning\": \"The reasoning the model used to identify the geographic location\",\\n    \"location_qid\": \"The Wikidata QID of the identified location\"\\n}\\n```'),\n",
       " LLMrequest(prompt_id='prompt-summary.txt', document_id='bpt6k852913n', prompt_path=PosixPath('/Users/mromanel/Documents/UniGe-TextEnt/chrono-spatial-processing/data/prompts/pregenerated/bpt6k852913n/bpt6k852913n_prompt-summary.txt'), prompt='Look at the following JSON object describing a theatre play in French (XVII century); the `metadata` property contains basic information about the play (author, title, publication date), while the `context` property contains information about the people and places that are most frequently mentioned in the play (such as label, mention frequency, and salient sentences where it appears).\\n\\nINPUT:\\n```json\\n{\\n  \"metadata\": {\\n    \"author\": \"Néel\",\\n    \"title\": \"L\\'illusion grotesque ou le point nécromancien, comédie / par M. Néel...\",\\n    \"publication_date\": \"1678\",\\n    \"document_id\": \"bpt6k852913n\"\\n  },\\n  \"context\": {\\n    \"people\": {\\n      \"top_1_person\": {\\n        \"entity\": {\\n          \"label\": \"Ormin\",\\n          \"frequency\": 2\\n        },\\n        \"related_sentences\": [\\n          \"Mais puis qu\\'il faut agir, agissons de concert, Et montrons qu\\'a plopos ce enaugemient nous sert5 La ruse en est subtile, et tout ce qui m\\'étonne C\\'est que Philandre encor de rien ne vous soupçonne, Et que toute l\\'intrigue est conduite à ce point Qu\\'il se voit presque pris, et ne le connaît point, \\\\nORMIN\\\\n Mais sachez que Crispin, comme j\\'ai pu l\\'instruire, En faisant le Devin a bien su se conduire, Et que par là Philandre est fort embarrassé Autant sur son hymen, que sur le temps passé.\",\\n          \"Quels devoirs nuit et jour le peuple me vient rendre, Et pour m\\'avoir déplu, combien j\\'en ai fait pendre? \\\\nORMIN\\\\n J\\'en ai dit encor plus afin de l\\'engager, Que tous vos sentiments n\\'allaient qu\\'à l\\'obliger;\",\\n          \"Oui, cela se peut faire aussi tôt qu\\'un Commis? \\\\nPHILANDRE\\\\n je veux uile en prenant un habillement gris. \\\\nORMIN\\\\n Et quel nom prendrez-vous? \\\\nPHILANDRE\\\\n Ou quelque autre à ton choix.\",\\n          \"\\\\nORMIN\\\\n Vous apprendrez que mon Maître Philandre, Qui se veut marier, ma fait malgré moi prendre Cette hient parure ce cet haait d\\'honneur, Pour être de ses vœux l\\'unique ambassadeur.\",\\n          \"Parle moi nettement, je veux que l\\'on s\\'explique, \\\\nORMIN\\\\n Un Savetier, Monsieur, parlant avec honheur, M\\'ayant montré de loin où votre Devineur Avait accoutumé de faire résidence.\"\\n        ]\\n      },\\n      \"top_5_persons\": [\\n        \"Ormin\",\\n        \"ARMINDE\",\\n        \"CRISPIN\",\\n        \"Beatrix\",\\n        \"Roselle\"\\n      ]\\n    },\\n    \"places\": {\\n      \"top_1_place\": {\\n        \"entity\": {\\n          \"label\": \"Senlis\",\\n          \"frequency\": 2\\n        },\\n        \"related_sentences\": [\\n          \"Elle voudrait déjà bien me tenir. \\\\nMASCARILLE\\\\n De grâce, Tant plus je m\\'étudie aux traits de votre face, E j Et plus je vois l\\'objet que j\\'ai vu dans Senlis Ne lésait-ce point vous ? \\\\nPHILANDRE\\\\n Non, mais bien à Paris Dedans le Luxembourg, et même aux Tuilleries, Je m\\'y suis signalé par mille batteries;\",\\n          \"J\\'arrive de Senlis pour épouser la fille, Tel que vous me voyez, de Monfieur Mascarille\"\\n        ]\\n      },\\n      \"top_5_places\": [\\n        \"Senlis\",\\n        \"Chalons en Champagne\",\\n        \"Paris\",\\n        \"Luxembourg\"\\n      ]\\n    }\\n  }\\n}\\n```\\n\\nYour role is to predict the location and historical period in which the action of the play is set. \\n\\nKEY RULES:\\n- Predict the timespan and not the precise and exact date of the period where the play could have taken place\\n- Do not write an introduction or summary \\n- The response must contain only valid JSON\\n- The values in the JSON \"timeframe_start\" and \"timeframe_end\" should always be a single valid date in the form [±Y]YYYY; negative values should be used for years before common era B.C.E. (e.g. `300 B.C.` should be represented as `-300`)\\n- if the provided information is not sufficient to determine historical period and/or location, the following values can be set to `None`: `period`, `timeframe_start`, `timeframe_end`, `location`, `location_qid`\\n\\nReturn your response and the underlying reasoning as a JSON object with the following structure:\\n```json\\n{\\n    \"period\": \"The historical period in which the play could have taken place\",\\n    \"period_reasoning\": \"The reasoning the model used to identify the historical period\",\\n    \"timeframe_start\": \"The start value of the historical period, formatted as [±Y]YYYY\",\\n    \"timeframe_end\": \"The end value of the historical period, formatted as [±Y]YYYY\",\\n    \"location\": \"The geographic location where the action of the play takes place\",\\n    \"location_reasoning\": \"The reasoning the model used to identify the geographic location\",\\n    \"location_qid\": \"The Wikidata QID of the identified location\"\\n}\\n```'),\n",
       " LLMrequest(prompt_id='prompt-metadata.txt', document_id='bpt6k852913n', prompt_path=PosixPath('/Users/mromanel/Documents/UniGe-TextEnt/chrono-spatial-processing/data/prompts/pregenerated/bpt6k852913n/bpt6k852913n_prompt-metadata.txt'), prompt='Look at the following JSON object describing a theatre play in French (XVII century); the `metadata` property contains basic information about the play (author, title, publication date).\\n\\nINPUT:\\n```json\\n{\\n  \"metadata\": {\\n    \"author\": \"Néel\",\\n    \"title\": \"L\\'illusion grotesque ou le point nécromancien, comédie / par M. Néel...\",\\n    \"publication_date\": \"1678\",\\n    \"document_id\": \"bpt6k852913n\"\\n  }\\n}\\n```\\n\\nYour role is to predict the location and historical period in which the action of the play is set. \\n\\nKEY RULES:\\n- Predict the timespan and not the precise and exact date of the period where the play could have taken place\\n- Do not write an introduction or summary \\n- The response must contain only valid JSON\\n- The values in the JSON \"timeframe_start\" and \"timeframe_end\" should always be a single valid date in the form [±Y]YYYY; negative values should be used for years before common era B.C.E. (e.g. `300 B.C.` should be represented as `-300`)\\n- if the provided information is not sufficient to determine historical period and/or location, the following values can be set to `None`: `period`, `timeframe_start`, `timeframe_end`, `location`, `location_qid`\\n\\nReturn your response and the underlying reasoning as a JSON object with the following structure:\\n```json\\n{\\n    \"period\": \"The historical period in which the play could have taken place\",\\n    \"period_reasoning\": \"The reasoning the model used to identify the historical period\",\\n    \"timeframe_start\": \"The start value of the historical period, formatted as [±Y]YYYY\",\\n    \"timeframe_end\": \"The end value of the historical period, formatted as [±Y]YYYY\",\\n    \"location\": \"The geographic location where the action of the play takes place\",\\n    \"location_reasoning\": \"The reasoning the model used to identify the geographic location\",\\n    \"location_qid\": \"The Wikidata QID of the identified location\"\\n}\\n```'),\n",
       " LLMrequest(prompt_id='prompt-excerpt.txt', document_id='bpt6k852913n', prompt_path=PosixPath('/Users/mromanel/Documents/UniGe-TextEnt/chrono-spatial-processing/data/prompts/pregenerated/bpt6k852913n/bpt6k852913n_prompt-excerpt.txt'), prompt='Look at the following JSON object describing a theatre play in French (XVII century); the `metadata` property contains basic information about the play (author, title, publication date), while the `excerpt` property contains an excerpt of 400 words sampled from around the middle of the document.\\n\\nINPUT:\\n```json\\n{\\n  \"metadata\": {\\n    \"author\": \"Néel\",\\n    \"title\": \"L\\'illusion grotesque ou le point nécromancien, comédie / par M. Néel...\",\\n    \"publication_date\": \"1678\",\\n    \"document_id\": \"bpt6k852913n\"\\n  },\\n  \"excerpt\": \"i, le voici qui s\\'avance; Qu\\'on nous laisse ici seuls. Ç ij SCÈNE III. PHIDADRE, IALEMON. \\\\nPHILANDRE\\\\n MOnseur, la connaissancs Que vous donne votre Ait des choses t ienir, Me fait vous consulter pour vous entretenir D\\'un scrupule importun dont la fin m\\'embarrasse. POLEMON. Il n\\'est rien que pour vous volontiers je ne fasse; Vous n\\'avez qu\\'à parler, et me prescrire en quoi Votre esprit inquiet veut\"\\n}\\n```\\n\\nYour role is to predict the location and historical period in which the action of the play is set. \\n\\nKEY RULES:\\n- Predict the timespan and not the precise and exact date of the period where the play could have taken place\\n- Do not write an introduction or summary \\n- The response must contain only valid JSON\\n- The values in the JSON \"timeframe_start\" and \"timeframe_end\" should always be a single valid date in the form [±Y]YYYY; negative values should be used for years before common era B.C.E. (e.g. `300 B.C.` should be represented as `-300`)\\n- if the provided information is not sufficient to determine historical period and/or location, the following values can be set to `None`: `period`, `timeframe_start`, `timeframe_end`, `location`, `location_qid`\\n\\nReturn your response and the underlying reasoning as a JSON object with the following structure:\\n```json\\n{\\n    \"period\": \"The historical period in which the play could have taken place\",\\n    \"period_reasoning\": \"The reasoning the model used to identify the historical period\",\\n    \"timeframe_start\": \"The start value of the historical period, formatted as [±Y]YYYY\",\\n    \"timeframe_end\": \"The end value of the historical period, formatted as [±Y]YYYY\",\\n    \"location\": \"The geographic location where the action of the play takes place\",\\n    \"location_reasoning\": \"The reasoning the model used to identify the geographic location\",\\n    \"location_qid\": \"The Wikidata QID of the identified location\"\\n}\\n```'),\n",
       " LLMrequest(prompt_id='prompt-summary.txt', document_id='bpt6k5772699f', prompt_path=PosixPath('/Users/mromanel/Documents/UniGe-TextEnt/chrono-spatial-processing/data/prompts/pregenerated/bpt6k5772699f/bpt6k5772699f_prompt-summary.txt'), prompt='Look at the following JSON object describing a theatre play in French (XVII century); the `metadata` property contains basic information about the play (author, title, publication date), while the `context` property contains information about the people and places that are most frequently mentioned in the play (such as label, mention frequency, and salient sentences where it appears).\\n\\nINPUT:\\n```json\\n{\\n  \"metadata\": {\\n    \"author\": \"Charenton, De\",\\n    \"title\": \"La mort de Baltazar, roy de Babilone , tragédie. Par le sieur de Charenton\",\\n    \"publication_date\": \"1662\",\\n    \"document_id\": \"bpt6k5772699f\"\\n  },\\n  \"context\": {\\n    \"people\": {\\n      \"top_1_person\": {\\n        \"entity\": {\\n          \"label\": \"Nabal\",\\n          \"frequency\": 3\\n        },\\n        \"related_sentences\": [\\n          \"Que votre bruit encor répond à votre audace, Que vous savez vanter ce qui n\\'est point de vous, Que votre humeur vous rend insupportable à rous, Que vous abusez trop du pouvoir qu\\'on vous donne, Que vous méconnaissez le sang de ma personne, Qu\\'en sin votre arrogance est venut à ce point Qu\\'un cœur comme le mien ne la sousfrira point, Qu\\'il faut à vos dépens. \\\\nNABAL\\\\n Ah, c\\'est trop vous entharé.\",\\n          \"Nsin par un malheur qui n\\'eût jamais d\\'égal, LI’de pour partie un Roi, mon amour et Nabal, Je me vois attaqué dans cetre dissérence, Que l\\'amour court au cœur, et laisse l\\'espérance, L\\'amour se sert de feux pour attaquer le cœur, Nabal par son crédit lui de sa douleur, Le Roi d\\'autorité répond à sa tristesse, Tous trois diversement causent ce qui me blesse, Que derermineray-je avec ce déplaisir?\",\\n          \"Nabal est sans défauts, homme dont les exploits Ont porté son mérite à s\\'égaller aux Rois, Il a foret cent sois, cent rempara, cent murailles, Combattu comme un Mars aumilieu des batailles, Fait redouter mon nom au bruit de ses combats, De Province en Province étendu mes Étars, Il doit à ce mérite et non à sa naissance, Son éclat et son rang qu\\'il tient pour recompence.\",\\n          \"Mais charmante beauré dans mon rigoureux sort, Malgré tous mes respects, Seigneur dois zaïre un éfort: Nabal dans son dessein m\\'oblige à me contraindre, M\\'oblige à vous parler, m\\'oblige A ne plus feindre: Il tire de mon cœur le secret de mes feux, Et pour le prévenir tu vous osefre mes vœux: Le met en est lâché, ma Divine Princesse, Mon cœur s\\'est découvert\",\\n          \". LE ROY, NAFAI naba caibe écectant par l\\'ordre du Ro LE ROY, As-tutont enten du \\\\nNABAL\\\\n Seigneur c\\'est ma douleur, J\\'apprends de son refus jusqu\\'où va mon malheur: Votre amitié combat, mais sa sierté l\\'emporte, Contre votre puissance elle devient plus forte, Et loin d\\'appréhender votre juste courroux, Se condamne elle -même à tomber sous ses coups.\"\\n        ]\\n      },\\n      \"top_5_persons\": [\\n        \"Nabal\",\\n        \"MISIA\",\\n        \"BALTAZAR\",\\n        \"Cyrus\",\\n        \"Arbas\"\\n      ]\\n    },\\n    \"places\": {\\n      \"top_1_place\": {\\n        \"entity\": {\\n          \"label\": \"Paris\",\\n          \"frequency\": 3\\n        },\\n        \"related_sentences\": [\\n          \"ledit Livre sans le consentement dudit Sieur de Charenton, à peine aux contrevenants de six mille livr ès d\\'amende, consiscation des exemplaires, et de tous dépens, dommages et intérêts, ainsi il est plus amplement porté par Priuilege Et ledit Sieur de Charenton a cédé nuant droict de Privilège à Nicolas Peingué et Jean Guignard fils, Marhands Libraires à Paris, pour en ouïr pendant ledit temps, suivant accordiaic entrait Entraient.\",\\n          \"Tout Paris admire que vous ne demeurez astachaît?  is Monde, que pour en être plus près, et pour lui communiquer vos assistances.\",\\n          \"grâce et Privilège du Roi donné à Paris\"\\n        ]\\n      },\\n      \"top_5_places\": [\\n        \"Paris\",\\n        \"PARIS\"\\n      ]\\n    }\\n  }\\n}\\n```\\n\\nYour role is to predict the location and historical period in which the action of the play is set. \\n\\nKEY RULES:\\n- Predict the timespan and not the precise and exact date of the period where the play could have taken place\\n- Do not write an introduction or summary \\n- The response must contain only valid JSON\\n- The values in the JSON \"timeframe_start\" and \"timeframe_end\" should always be a single valid date in the form [±Y]YYYY; negative values should be used for years before common era B.C.E. (e.g. `300 B.C.` should be represented as `-300`)\\n- if the provided information is not sufficient to determine historical period and/or location, the following values can be set to `None`: `period`, `timeframe_start`, `timeframe_end`, `location`, `location_qid`\\n\\nReturn your response and the underlying reasoning as a JSON object with the following structure:\\n```json\\n{\\n    \"period\": \"The historical period in which the play could have taken place\",\\n    \"period_reasoning\": \"The reasoning the model used to identify the historical period\",\\n    \"timeframe_start\": \"The start value of the historical period, formatted as [±Y]YYYY\",\\n    \"timeframe_end\": \"The end value of the historical period, formatted as [±Y]YYYY\",\\n    \"location\": \"The geographic location where the action of the play takes place\",\\n    \"location_reasoning\": \"The reasoning the model used to identify the geographic location\",\\n    \"location_qid\": \"The Wikidata QID of the identified location\"\\n}\\n```'),\n",
       " LLMrequest(prompt_id='prompt-metadata.txt', document_id='bpt6k5772699f', prompt_path=PosixPath('/Users/mromanel/Documents/UniGe-TextEnt/chrono-spatial-processing/data/prompts/pregenerated/bpt6k5772699f/bpt6k5772699f_prompt-metadata.txt'), prompt='Look at the following JSON object describing a theatre play in French (XVII century); the `metadata` property contains basic information about the play (author, title, publication date).\\n\\nINPUT:\\n```json\\n{\\n  \"metadata\": {\\n    \"author\": \"Charenton, De\",\\n    \"title\": \"La mort de Baltazar, roy de Babilone , tragédie. Par le sieur de Charenton\",\\n    \"publication_date\": \"1662\",\\n    \"document_id\": \"bpt6k5772699f\"\\n  }\\n}\\n```\\n\\nYour role is to predict the location and historical period in which the action of the play is set. \\n\\nKEY RULES:\\n- Predict the timespan and not the precise and exact date of the period where the play could have taken place\\n- Do not write an introduction or summary \\n- The response must contain only valid JSON\\n- The values in the JSON \"timeframe_start\" and \"timeframe_end\" should always be a single valid date in the form [±Y]YYYY; negative values should be used for years before common era B.C.E. (e.g. `300 B.C.` should be represented as `-300`)\\n- if the provided information is not sufficient to determine historical period and/or location, the following values can be set to `None`: `period`, `timeframe_start`, `timeframe_end`, `location`, `location_qid`\\n\\nReturn your response and the underlying reasoning as a JSON object with the following structure:\\n```json\\n{\\n    \"period\": \"The historical period in which the play could have taken place\",\\n    \"period_reasoning\": \"The reasoning the model used to identify the historical period\",\\n    \"timeframe_start\": \"The start value of the historical period, formatted as [±Y]YYYY\",\\n    \"timeframe_end\": \"The end value of the historical period, formatted as [±Y]YYYY\",\\n    \"location\": \"The geographic location where the action of the play takes place\",\\n    \"location_reasoning\": \"The reasoning the model used to identify the geographic location\",\\n    \"location_qid\": \"The Wikidata QID of the identified location\"\\n}\\n```'),\n",
       " LLMrequest(prompt_id='prompt-excerpt.txt', document_id='bpt6k5772699f', prompt_path=PosixPath('/Users/mromanel/Documents/UniGe-TextEnt/chrono-spatial-processing/data/prompts/pregenerated/bpt6k5772699f/bpt6k5772699f_prompt-excerpt.txt'), prompt='Look at the following JSON object describing a theatre play in French (XVII century); the `metadata` property contains basic information about the play (author, title, publication date), while the `excerpt` property contains an excerpt of 400 words sampled from around the middle of the document.\\n\\nINPUT:\\n```json\\n{\\n  \"metadata\": {\\n    \"author\": \"Charenton, De\",\\n    \"title\": \"La mort de Baltazar, roy de Babilone , tragédie. Par le sieur de Charenton\",\\n    \"publication_date\": \"1662\",\\n    \"document_id\": \"bpt6k5772699f\"\\n  },\\n  \"excerpt\": \"onge vous étonne, Une vapeur de nuit ne peut nuire à personne: Bannissez, bannissez cette vaine terreur, Qui cause votre crainte, et vous abat le cœur. Quel danger courons-nous dedans nos sorteresses? Lln dou A daus Celui d\\'être surpris pendant nos allégresses LE ROY. Quoi vous craignez Cyrus dans l\\'heure du festin? À l\\'abri de nos murs Seigneur brave le destin: Il faut pour gagner l\\'antre, il fau\"\\n}\\n```\\n\\nYour role is to predict the location and historical period in which the action of the play is set. \\n\\nKEY RULES:\\n- Predict the timespan and not the precise and exact date of the period where the play could have taken place\\n- Do not write an introduction or summary \\n- The response must contain only valid JSON\\n- The values in the JSON \"timeframe_start\" and \"timeframe_end\" should always be a single valid date in the form [±Y]YYYY; negative values should be used for years before common era B.C.E. (e.g. `300 B.C.` should be represented as `-300`)\\n- if the provided information is not sufficient to determine historical period and/or location, the following values can be set to `None`: `period`, `timeframe_start`, `timeframe_end`, `location`, `location_qid`\\n\\nReturn your response and the underlying reasoning as a JSON object with the following structure:\\n```json\\n{\\n    \"period\": \"The historical period in which the play could have taken place\",\\n    \"period_reasoning\": \"The reasoning the model used to identify the historical period\",\\n    \"timeframe_start\": \"The start value of the historical period, formatted as [±Y]YYYY\",\\n    \"timeframe_end\": \"The end value of the historical period, formatted as [±Y]YYYY\",\\n    \"location\": \"The geographic location where the action of the play takes place\",\\n    \"location_reasoning\": \"The reasoning the model used to identify the geographic location\",\\n    \"location_qid\": \"The Wikidata QID of the identified location\"\\n}\\n```'),\n",
       " LLMrequest(prompt_id='prompt-summary.txt', document_id='bpt6k9807756q', prompt_path=PosixPath('/Users/mromanel/Documents/UniGe-TextEnt/chrono-spatial-processing/data/prompts/pregenerated/bpt6k9807756q/bpt6k9807756q_prompt-summary.txt'), prompt='Look at the following JSON object describing a theatre play in French (XVII century); the `metadata` property contains basic information about the play (author, title, publication date), while the `context` property contains information about the people and places that are most frequently mentioned in the play (such as label, mention frequency, and salient sentences where it appears).\\n\\nINPUT:\\n```json\\n{\\n  \"metadata\": {\\n    \"author\": \"Genest, Charles-Claude\",\\n    \"title\": \"Zelonide, princesse de Sparte . Tragedie\",\\n    \"publication_date\": \"1682\",\\n    \"document_id\": \"bpt6k9807756q\"\\n  },\\n  \"context\": {\\n    \"people\": {\\n      \"top_1_person\": {\\n        \"entity\": {\\n          \"label\": \"Pyrrhus\",\\n          \"frequency\": 52\\n        },\\n        \"related_sentences\": [\\n          \"Mais cette jeune ardeur qui vous porte aux combats, Seigneur, aurait besoin d\\'armes et de Soldats Malgré ces hauts désirs notre Ville déserte Sans pouvoir se défendre à Pyrrhus est ouverte.\",\\n          \"Il va chercher Pyrrhus, en implorer l\\'appui, J\\'y consens, je le porte à traiter avec lui, Il réussit.\",\\n          \"Aux Armes de Pyrrhus sans défense livrée, Sa honte est infaillible, ou sa perte assurée.\",\\n          \"Quand Pyrrhus nous ferait succomber, Tout l\\'État avec nous n\\'est pas prêt à tomber.\",\\n          \"si le fier Pyrrhus ose nous outrager, Ne délibérons point, et courons nous venger;\"\\n        ]\\n      },\\n      \"top_5_persons\": [\\n        \"Pyrrhus\",\\n        \"Pyrrhus\",\\n        \"Pyrrhus\",\\n        \"Phillus\",\\n        \"Spartiates\"\\n      ]\\n    },\\n    \"places\": {\\n      \"top_1_place\": {\\n        \"entity\": {\\n          \"label\": \"Sparte\",\\n          \"frequency\": 52\\n        },\\n        \"related_sentences\": [\\n          \"D\\'autres ont prétendu disputer à Zélonide le titre de parfaite Héroïne Outre que la perfection absolue n\\'est pas toujours nécessaire aux Héros de la Tragédie, j\\'ai à répondre encore qu\\'on ne sait pas bien toutes les circonstances de la rupture de Zélonide avec Cléonime, et de son engagement avec Acorate mais que toutes les langes qu\\'on lui donne à Sparte, et les aclamations que font pour elle tant de Sages Vieillards, montrent assez qu\\'ils la regardaient comme une Princesse Héroïque:\",\\n          \"Mais ce qui était inconnu à Sparte, et en quoi vous l\\'emportez sans doute sur Elles, c\\'est d\\'avoir toute la grandeur et toute l\\'élévation de leurs sentiments, sans rien perdre de cette charmante douceur, et de cette délicate bienséance qui sont si propres à nêtre Sexe, et qui sont le dernier trait, et l\\'accomplissement des Grâces et des Vertus.\",\\n          \"Que Sparte toujours libre, et toujours souveraine, Pouponhonreloi ne tut pon raccepter, Et pour son Ennemi ne peut le redouter. \\\\nLISIMACUS\\\\n Vous quitterez bien tôt cet orguëil téméraire, Quand vous verrez sur vous éclater la colère D\\'un Roi que sa clémence a voulu retenir, Et tel qu\\'un Dieu vengeur forcé de vous punir. \\\\nACORATE\\\\n \",\\n          \"Mon Père et nos Guerriers qu\\'à luivis la victoire, Reviendront venger Sparte, et rétablir sa gloire, Ou se feront enfin un Thrône et des Remparts, Par Part où s\\'étendra la pointe de leurs dards. \\\\nPHILUS\\\\n Que nos Enfants il eners, nos illustres Spartaines Évitent promtement et la mort et les chaînes.\",\\n          \"C\\'est pour la proposer que vient Lisimacus Il tient le premier rang à la Cour de Pirrhus Songez en était parlant au pouvoir de son maître, Songez qu\\'un Camp nombreux dans nos champs va paraître, Que Sparte aux Ennemis s\\'ouvre de toutes parts, Vuide des Habitants qui sont ses seuls Remparts.\"\\n        ]\\n      },\\n      \"top_5_places\": [\\n        \"Sparte\",\\n        \"Grèce\",\\n        \"France\",\\n        \"Paris\",\\n        \"Rome\"\\n      ]\\n    }\\n  }\\n}\\n```\\n\\nYour role is to predict the location and historical period in which the action of the play is set. \\n\\nKEY RULES:\\n- Predict the timespan and not the precise and exact date of the period where the play could have taken place\\n- Do not write an introduction or summary \\n- The response must contain only valid JSON\\n- The values in the JSON \"timeframe_start\" and \"timeframe_end\" should always be a single valid date in the form [±Y]YYYY; negative values should be used for years before common era B.C.E. (e.g. `300 B.C.` should be represented as `-300`)\\n- if the provided information is not sufficient to determine historical period and/or location, the following values can be set to `None`: `period`, `timeframe_start`, `timeframe_end`, `location`, `location_qid`\\n\\nReturn your response and the underlying reasoning as a JSON object with the following structure:\\n```json\\n{\\n    \"period\": \"The historical period in which the play could have taken place\",\\n    \"period_reasoning\": \"The reasoning the model used to identify the historical period\",\\n    \"timeframe_start\": \"The start value of the historical period, formatted as [±Y]YYYY\",\\n    \"timeframe_end\": \"The end value of the historical period, formatted as [±Y]YYYY\",\\n    \"location\": \"The geographic location where the action of the play takes place\",\\n    \"location_reasoning\": \"The reasoning the model used to identify the geographic location\",\\n    \"location_qid\": \"The Wikidata QID of the identified location\"\\n}\\n```'),\n",
       " LLMrequest(prompt_id='prompt-metadata.txt', document_id='bpt6k9807756q', prompt_path=PosixPath('/Users/mromanel/Documents/UniGe-TextEnt/chrono-spatial-processing/data/prompts/pregenerated/bpt6k9807756q/bpt6k9807756q_prompt-metadata.txt'), prompt='Look at the following JSON object describing a theatre play in French (XVII century); the `metadata` property contains basic information about the play (author, title, publication date).\\n\\nINPUT:\\n```json\\n{\\n  \"metadata\": {\\n    \"author\": \"Genest, Charles-Claude\",\\n    \"title\": \"Zelonide, princesse de Sparte . Tragedie\",\\n    \"publication_date\": \"1682\",\\n    \"document_id\": \"bpt6k9807756q\"\\n  }\\n}\\n```\\n\\nYour role is to predict the location and historical period in which the action of the play is set. \\n\\nKEY RULES:\\n- Predict the timespan and not the precise and exact date of the period where the play could have taken place\\n- Do not write an introduction or summary \\n- The response must contain only valid JSON\\n- The values in the JSON \"timeframe_start\" and \"timeframe_end\" should always be a single valid date in the form [±Y]YYYY; negative values should be used for years before common era B.C.E. (e.g. `300 B.C.` should be represented as `-300`)\\n- if the provided information is not sufficient to determine historical period and/or location, the following values can be set to `None`: `period`, `timeframe_start`, `timeframe_end`, `location`, `location_qid`\\n\\nReturn your response and the underlying reasoning as a JSON object with the following structure:\\n```json\\n{\\n    \"period\": \"The historical period in which the play could have taken place\",\\n    \"period_reasoning\": \"The reasoning the model used to identify the historical period\",\\n    \"timeframe_start\": \"The start value of the historical period, formatted as [±Y]YYYY\",\\n    \"timeframe_end\": \"The end value of the historical period, formatted as [±Y]YYYY\",\\n    \"location\": \"The geographic location where the action of the play takes place\",\\n    \"location_reasoning\": \"The reasoning the model used to identify the geographic location\",\\n    \"location_qid\": \"The Wikidata QID of the identified location\"\\n}\\n```'),\n",
       " LLMrequest(prompt_id='prompt-excerpt.txt', document_id='bpt6k9807756q', prompt_path=PosixPath('/Users/mromanel/Documents/UniGe-TextEnt/chrono-spatial-processing/data/prompts/pregenerated/bpt6k9807756q/bpt6k9807756q_prompt-excerpt.txt'), prompt='Look at the following JSON object describing a theatre play in French (XVII century); the `metadata` property contains basic information about the play (author, title, publication date), while the `excerpt` property contains an excerpt of 400 words sampled from around the middle of the document.\\n\\nINPUT:\\n```json\\n{\\n  \"metadata\": {\\n    \"author\": \"Genest, Charles-Claude\",\\n    \"title\": \"Zelonide, princesse de Sparte . Tragedie\",\\n    \"publication_date\": \"1682\",\\n    \"document_id\": \"bpt6k9807756q\"\\n  },\\n  \"excerpt\": \"mmes animées, A de plus beaux desseins Sparte nous a formées. Loin de vous retenir par des charmes trompeurs, Sans cesse aux grands exploits nous enflammons vos cœurs. Nous voulons que l\\'amour les embrase et les guide Pour prendre vers la gloire un essor plus rapide. Quelle outrageuse erreur vous fait donc présumer Qu\\'ici nous vous nuirons, loin de vous animer? Aider à soutenir un Thrône qui chanc\"\\n}\\n```\\n\\nYour role is to predict the location and historical period in which the action of the play is set. \\n\\nKEY RULES:\\n- Predict the timespan and not the precise and exact date of the period where the play could have taken place\\n- Do not write an introduction or summary \\n- The response must contain only valid JSON\\n- The values in the JSON \"timeframe_start\" and \"timeframe_end\" should always be a single valid date in the form [±Y]YYYY; negative values should be used for years before common era B.C.E. (e.g. `300 B.C.` should be represented as `-300`)\\n- if the provided information is not sufficient to determine historical period and/or location, the following values can be set to `None`: `period`, `timeframe_start`, `timeframe_end`, `location`, `location_qid`\\n\\nReturn your response and the underlying reasoning as a JSON object with the following structure:\\n```json\\n{\\n    \"period\": \"The historical period in which the play could have taken place\",\\n    \"period_reasoning\": \"The reasoning the model used to identify the historical period\",\\n    \"timeframe_start\": \"The start value of the historical period, formatted as [±Y]YYYY\",\\n    \"timeframe_end\": \"The end value of the historical period, formatted as [±Y]YYYY\",\\n    \"location\": \"The geographic location where the action of the play takes place\",\\n    \"location_reasoning\": \"The reasoning the model used to identify the geographic location\",\\n    \"location_qid\": \"The Wikidata QID of the identified location\"\\n}\\n```')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/mromanel/Documents/UniGe-TextEnt/chrono-spatial-processing/data/validation/llm_responses')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_llm_responses_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the LLM responses before generating new ones \n",
    "def clean_up_directory(directory_path: Path) -> None:\n",
    "    for item in directory_path.iterdir():\n",
    "        if item.is_dir():\n",
    "            print(f'Removed folder {item} and all its contents')\n",
    "            shutil.rmtree(item)\n",
    "        else:\n",
    "            print(f'Removed file {item}')\n",
    "            item.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_up_directory(validation_llm_responses_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ai.Client()\n",
    "client.configure({\"ollama\" : {\"timeout\": 600}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ollama:phi4-mini:latest',\n",
       " 'ollama:gemma3:12b',\n",
       " 'ollama:mistral-small:24b',\n",
       " 'ollama:deepseek-r1:14b',\n",
       " 'ollama:deepseek-r1:32b',\n",
       " 'openai:o1-mini',\n",
       " 'openai:gpt-4o',\n",
       " 'deepseek:deepseek-reasoner',\n",
       " 'anthropic:claude-3-7-sonnet-20250219']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping ollama:phi4-mini:latest\n",
      "Skipping ollama:gemma3:12b\n",
      "Skipping ollama:mistral-small:24b\n",
      "Skipping ollama:deepseek-r1:14b\n",
      "Skipping ollama:deepseek-r1:32b\n",
      "Processing prompt prompt-excerpt.txt for document bpt6k10901623 using model openai:o1-mini (temp=None)\n",
      "Time taken to get response: 7.35 seconds. Total tokens: 1427\n",
      "Processing prompt prompt-metadata.txt for document bpt6k10901623 using model openai:o1-mini (temp=None)\n",
      "Time taken to get response: 7.43 seconds. Total tokens: 1278\n",
      "Processing prompt prompt-summary.txt for document bpt6k10901623 using model openai:o1-mini (temp=None)\n",
      "Time taken to get response: 7.94 seconds. Total tokens: 2290\n",
      "Processing prompt prompt-excerpt.txt for document bpt6k1090242p using model openai:o1-mini (temp=None)\n",
      "Time taken to get response: 9.95 seconds. Total tokens: 2094\n",
      "Processing prompt prompt-metadata.txt for document bpt6k1090242p using model openai:o1-mini (temp=None)\n",
      "Time taken to get response: 9.10 seconds. Total tokens: 1236\n",
      "Processing prompt prompt-summary.txt for document bpt6k1090242p using model openai:o1-mini (temp=None)\n",
      "Time taken to get response: 9.44 seconds. Total tokens: 2942\n",
      "Processing prompt prompt-summary.txt for document bpt6k852913n using model openai:o1-mini (temp=None)\n",
      "Time taken to get response: 5.72 seconds. Total tokens: 2133\n",
      "Processing prompt prompt-metadata.txt for document bpt6k852913n using model openai:o1-mini (temp=None)\n",
      "Time taken to get response: 4.33 seconds. Total tokens: 1089\n",
      "Processing prompt prompt-excerpt.txt for document bpt6k852913n using model openai:o1-mini (temp=None)\n",
      "Time taken to get response: 5.62 seconds. Total tokens: 1570\n",
      "Processing prompt prompt-summary.txt for document bpt6k5772699f using model openai:o1-mini (temp=None)\n",
      "Time taken to get response: 5.51 seconds. Total tokens: 2134\n",
      "Processing prompt prompt-metadata.txt for document bpt6k5772699f using model openai:o1-mini (temp=None)\n",
      "Time taken to get response: 6.16 seconds. Total tokens: 1082\n",
      "Processing prompt prompt-excerpt.txt for document bpt6k5772699f using model openai:o1-mini (temp=None)\n",
      "Time taken to get response: 8.04 seconds. Total tokens: 1115\n",
      "Processing prompt prompt-summary.txt for document bpt6k9807756q using model openai:o1-mini (temp=None)\n",
      "Time taken to get response: 5.39 seconds. Total tokens: 2011\n",
      "Processing prompt prompt-metadata.txt for document bpt6k9807756q using model openai:o1-mini (temp=None)\n",
      "Time taken to get response: 6.17 seconds. Total tokens: 1300\n",
      "Processing prompt prompt-excerpt.txt for document bpt6k9807756q using model openai:o1-mini (temp=None)\n",
      "Time taken to get response: 6.66 seconds. Total tokens: 1547\n",
      "Processing prompt prompt-excerpt.txt for document bpt6k10901623 using model openai:gpt-4o (temp=0.2)\n",
      "Time taken to get response: 3.70 seconds. Total tokens: 725\n",
      "Processing prompt prompt-metadata.txt for document bpt6k10901623 using model openai:gpt-4o (temp=0.2)\n",
      "Time taken to get response: 3.53 seconds. Total tokens: 616\n",
      "Processing prompt prompt-summary.txt for document bpt6k10901623 using model openai:gpt-4o (temp=0.2)\n",
      "Time taken to get response: 2.80 seconds. Total tokens: 1263\n",
      "Processing prompt prompt-excerpt.txt for document bpt6k1090242p using model openai:gpt-4o (temp=0.2)\n",
      "Time taken to get response: 5.06 seconds. Total tokens: 753\n",
      "Processing prompt prompt-metadata.txt for document bpt6k1090242p using model openai:gpt-4o (temp=0.2)\n",
      "Time taken to get response: 4.67 seconds. Total tokens: 610\n",
      "Processing prompt prompt-summary.txt for document bpt6k1090242p using model openai:gpt-4o (temp=0.2)\n",
      "Time taken to get response: 3.27 seconds. Total tokens: 1714\n",
      "Processing prompt prompt-summary.txt for document bpt6k852913n using model openai:gpt-4o (temp=0.2)\n",
      "Time taken to get response: 4.36 seconds. Total tokens: 1312\n",
      "Processing prompt prompt-metadata.txt for document bpt6k852913n using model openai:gpt-4o (temp=0.2)\n",
      "Time taken to get response: 3.10 seconds. Total tokens: 607\n",
      "Processing prompt prompt-excerpt.txt for document bpt6k852913n using model openai:gpt-4o (temp=0.2)\n",
      "Time taken to get response: 4.25 seconds. Total tokens: 755\n",
      "Processing prompt prompt-summary.txt for document bpt6k5772699f using model openai:gpt-4o (temp=0.2)\n",
      "Time taken to get response: 3.60 seconds. Total tokens: 1504\n",
      "Processing prompt prompt-metadata.txt for document bpt6k5772699f using model openai:gpt-4o (temp=0.2)\n",
      "Time taken to get response: 3.12 seconds. Total tokens: 607\n",
      "Processing prompt prompt-excerpt.txt for document bpt6k5772699f using model openai:gpt-4o (temp=0.2)\n",
      "Time taken to get response: 4.00 seconds. Total tokens: 772\n",
      "Processing prompt prompt-summary.txt for document bpt6k9807756q using model openai:gpt-4o (temp=0.2)\n",
      "Time taken to get response: 3.27 seconds. Total tokens: 1467\n",
      "Processing prompt prompt-metadata.txt for document bpt6k9807756q using model openai:gpt-4o (temp=0.2)\n",
      "Time taken to get response: 6.75 seconds. Total tokens: 597\n",
      "Processing prompt prompt-excerpt.txt for document bpt6k9807756q using model openai:gpt-4o (temp=0.2)\n",
      "Time taken to get response: 2.84 seconds. Total tokens: 703\n",
      "Skipping request for document bpt6k10901623[prompt-excerpt.txt] using model deepseek:deepseek-reasoner as it already exists\n",
      "Skipping request for document bpt6k10901623[prompt-metadata.txt] using model deepseek:deepseek-reasoner as it already exists\n",
      "Skipping request for document bpt6k10901623[prompt-summary.txt] using model deepseek:deepseek-reasoner as it already exists\n",
      "Skipping request for document bpt6k1090242p[prompt-excerpt.txt] using model deepseek:deepseek-reasoner as it already exists\n",
      "Skipping request for document bpt6k1090242p[prompt-metadata.txt] using model deepseek:deepseek-reasoner as it already exists\n",
      "Skipping request for document bpt6k1090242p[prompt-summary.txt] using model deepseek:deepseek-reasoner as it already exists\n",
      "Skipping request for document bpt6k852913n[prompt-summary.txt] using model deepseek:deepseek-reasoner as it already exists\n",
      "Skipping request for document bpt6k852913n[prompt-metadata.txt] using model deepseek:deepseek-reasoner as it already exists\n",
      "Skipping request for document bpt6k852913n[prompt-excerpt.txt] using model deepseek:deepseek-reasoner as it already exists\n",
      "Skipping request for document bpt6k5772699f[prompt-summary.txt] using model deepseek:deepseek-reasoner as it already exists\n",
      "Skipping request for document bpt6k5772699f[prompt-metadata.txt] using model deepseek:deepseek-reasoner as it already exists\n",
      "Skipping request for document bpt6k5772699f[prompt-excerpt.txt] using model deepseek:deepseek-reasoner as it already exists\n",
      "Skipping request for document bpt6k9807756q[prompt-summary.txt] using model deepseek:deepseek-reasoner as it already exists\n",
      "Skipping request for document bpt6k9807756q[prompt-metadata.txt] using model deepseek:deepseek-reasoner as it already exists\n",
      "Skipping request for document bpt6k9807756q[prompt-excerpt.txt] using model deepseek:deepseek-reasoner as it already exists\n",
      "Skipping request for document bpt6k10901623[prompt-excerpt.txt] using model anthropic:claude-3-7-sonnet-20250219 as it already exists\n",
      "Skipping request for document bpt6k10901623[prompt-metadata.txt] using model anthropic:claude-3-7-sonnet-20250219 as it already exists\n",
      "Skipping request for document bpt6k10901623[prompt-summary.txt] using model anthropic:claude-3-7-sonnet-20250219 as it already exists\n",
      "Skipping request for document bpt6k1090242p[prompt-excerpt.txt] using model anthropic:claude-3-7-sonnet-20250219 as it already exists\n",
      "Skipping request for document bpt6k1090242p[prompt-metadata.txt] using model anthropic:claude-3-7-sonnet-20250219 as it already exists\n",
      "Skipping request for document bpt6k1090242p[prompt-summary.txt] using model anthropic:claude-3-7-sonnet-20250219 as it already exists\n",
      "Skipping request for document bpt6k852913n[prompt-summary.txt] using model anthropic:claude-3-7-sonnet-20250219 as it already exists\n",
      "Skipping request for document bpt6k852913n[prompt-metadata.txt] using model anthropic:claude-3-7-sonnet-20250219 as it already exists\n",
      "Skipping request for document bpt6k852913n[prompt-excerpt.txt] using model anthropic:claude-3-7-sonnet-20250219 as it already exists\n",
      "Processing prompt prompt-summary.txt for document bpt6k5772699f using model anthropic:claude-3-7-sonnet-20250219 (temp=None)\n",
      "Time taken to get response: 5.35 seconds. Total tokens: 1896\n",
      "Processing prompt prompt-metadata.txt for document bpt6k5772699f using model anthropic:claude-3-7-sonnet-20250219 (temp=None)\n",
      "Time taken to get response: 4.05 seconds. Total tokens: 720\n",
      "Processing prompt prompt-excerpt.txt for document bpt6k5772699f using model anthropic:claude-3-7-sonnet-20250219 (temp=None)\n",
      "Time taken to get response: 5.96 seconds. Total tokens: 970\n",
      "Processing prompt prompt-summary.txt for document bpt6k9807756q using model anthropic:claude-3-7-sonnet-20250219 (temp=None)\n",
      "Time taken to get response: 6.20 seconds. Total tokens: 1870\n",
      "Processing prompt prompt-metadata.txt for document bpt6k9807756q using model anthropic:claude-3-7-sonnet-20250219 (temp=None)\n",
      "Time taken to get response: 4.56 seconds. Total tokens: 723\n",
      "Processing prompt prompt-excerpt.txt for document bpt6k9807756q using model anthropic:claude-3-7-sonnet-20250219 (temp=None)\n",
      "Time taken to get response: 4.22 seconds. Total tokens: 869\n"
     ]
    }
   ],
   "source": [
    "llm_responses = []\n",
    "reasoning_llms = ['openai:o1-mini', 'deepseek:deepseek-reasoner', 'anthropic:claude-3-7-sonnet-20250219']\n",
    "default_temperature = config['validation']['temperature']\n",
    "\n",
    "for model in llms:\n",
    "    if model.startswith('ollama'):\n",
    "        print(f'Skipping {model}')\n",
    "        continue\n",
    "    if model in reasoning_llms:\n",
    "        llm_responses += query_llm(client, model, llm_requests, validation_llm_responses_path)\n",
    "    else:\n",
    "        llm_responses += query_llm(client, model, llm_requests, validation_llm_responses_path, temperature=default_temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask LLM-judge\n",
    "\n",
    "- get list of LLM judges (from config file)\n",
    "- for each model, get score predictions and save to a dataframe/csv file (`data/validation/scores`)\n",
    "    - save both the predictions (scores + reasons) and the unpacked scores (for further processing) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_llm_responses_path = Path(base_path / config['validation']['responses_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"['Unnamed: 11', 'Unnamed: 12'] not found in axis\"\n"
     ]
    }
   ],
   "source": [
    "# this should return only the test (validation) set (n=5 docs)\n",
    "validation_docs, df_validation_data = prepare_evaluation_dataframe(\n",
    "    llm_response_path=validation_llm_responses_path,\n",
    "    gt_annotations_path=gt_path,\n",
    "    gt_metadata_path=gt_path,\n",
    "    split=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>model_name</th>\n",
       "      <th>document_id</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>document_length</th>\n",
       "      <th>keep_fine_tuning</th>\n",
       "      <th>gt_period</th>\n",
       "      <th>pred_period</th>\n",
       "      <th>...</th>\n",
       "      <th>gt_preferred_location</th>\n",
       "      <th>gt_accepted_locations</th>\n",
       "      <th>pred_location</th>\n",
       "      <th>score_location_string</th>\n",
       "      <th>gt_preferred_location_QID</th>\n",
       "      <th>gt_acceptable_location_QIDs</th>\n",
       "      <th>pred_location_qid</th>\n",
       "      <th>score_location_qid</th>\n",
       "      <th>gt_location_reason</th>\n",
       "      <th>pred_location_reasoning</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>response_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bpt6k10901623$prompt-excerpt.txt$ollama:mistral-small:24b</th>\n",
       "      <td>prompt-excerpt.txt</td>\n",
       "      <td>ollama:mistral-small:24b</td>\n",
       "      <td>bpt6k10901623</td>\n",
       "      <td>Boisrobert, François de</td>\n",
       "      <td>Théodore, Reyne de Hongrie, tragi-comédie</td>\n",
       "      <td>1658</td>\n",
       "      <td>80779</td>\n",
       "      <td>True</td>\n",
       "      <td>Middle Ages</td>\n",
       "      <td>Middle Ages</td>\n",
       "      <td>...</td>\n",
       "      <td>Székesfehérvár</td>\n",
       "      <td>Székesfehérvár | Albe royale | Alba Regia | Hu...</td>\n",
       "      <td>Hungary</td>\n",
       "      <td>None</td>\n",
       "      <td>Q130212</td>\n",
       "      <td>Q130212 | Q28</td>\n",
       "      <td>Q27</td>\n",
       "      <td>None</td>\n",
       "      <td>- name of the town: Albe royale - King and que...</td>\n",
       "      <td>The title explicitly mentions 'Hongrie', which...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bpt6k10901623$prompt-summary.txt$openai:o1-mini</th>\n",
       "      <td>prompt-summary.txt</td>\n",
       "      <td>openai:o1-mini</td>\n",
       "      <td>bpt6k10901623</td>\n",
       "      <td>Boisrobert, François de</td>\n",
       "      <td>Théodore, Reyne de Hongrie, tragi-comédie</td>\n",
       "      <td>1658</td>\n",
       "      <td>80779</td>\n",
       "      <td>True</td>\n",
       "      <td>Middle Ages</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>Székesfehérvár</td>\n",
       "      <td>Székesfehérvár | Albe royale | Alba Regia | Hu...</td>\n",
       "      <td>Hungary</td>\n",
       "      <td>None</td>\n",
       "      <td>Q130212</td>\n",
       "      <td>Q130212 | Q28</td>\n",
       "      <td>Q28</td>\n",
       "      <td>None</td>\n",
       "      <td>- name of the town: Albe royale - King and que...</td>\n",
       "      <td>The title of the play refers to 'Reyne de Hong...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bpt6k10901623$prompt-metadata.txt$openai:gpt-4o</th>\n",
       "      <td>prompt-metadata.txt</td>\n",
       "      <td>openai:gpt-4o</td>\n",
       "      <td>bpt6k10901623</td>\n",
       "      <td>Boisrobert, François de</td>\n",
       "      <td>Théodore, Reyne de Hongrie, tragi-comédie</td>\n",
       "      <td>1658</td>\n",
       "      <td>80779</td>\n",
       "      <td>True</td>\n",
       "      <td>Middle Ages</td>\n",
       "      <td>Middle Ages</td>\n",
       "      <td>...</td>\n",
       "      <td>Székesfehérvár</td>\n",
       "      <td>Székesfehérvár | Albe royale | Alba Regia | Hu...</td>\n",
       "      <td>Kingdom of Hungary</td>\n",
       "      <td>None</td>\n",
       "      <td>Q130212</td>\n",
       "      <td>Q130212 | Q28</td>\n",
       "      <td>Q28</td>\n",
       "      <td>None</td>\n",
       "      <td>- name of the town: Albe royale - King and que...</td>\n",
       "      <td>The title explicitly mentions 'Reyne de Hongri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              prompt_id  \\\n",
       "response_id                                                               \n",
       "bpt6k10901623$prompt-excerpt.txt$ollama:mistral...   prompt-excerpt.txt   \n",
       "bpt6k10901623$prompt-summary.txt$openai:o1-mini      prompt-summary.txt   \n",
       "bpt6k10901623$prompt-metadata.txt$openai:gpt-4o     prompt-metadata.txt   \n",
       "\n",
       "                                                                  model_name  \\\n",
       "response_id                                                                    \n",
       "bpt6k10901623$prompt-excerpt.txt$ollama:mistral...  ollama:mistral-small:24b   \n",
       "bpt6k10901623$prompt-summary.txt$openai:o1-mini               openai:o1-mini   \n",
       "bpt6k10901623$prompt-metadata.txt$openai:gpt-4o                openai:gpt-4o   \n",
       "\n",
       "                                                      document_id  \\\n",
       "response_id                                                         \n",
       "bpt6k10901623$prompt-excerpt.txt$ollama:mistral...  bpt6k10901623   \n",
       "bpt6k10901623$prompt-summary.txt$openai:o1-mini     bpt6k10901623   \n",
       "bpt6k10901623$prompt-metadata.txt$openai:gpt-4o     bpt6k10901623   \n",
       "\n",
       "                                                                     author  \\\n",
       "response_id                                                                   \n",
       "bpt6k10901623$prompt-excerpt.txt$ollama:mistral...  Boisrobert, François de   \n",
       "bpt6k10901623$prompt-summary.txt$openai:o1-mini     Boisrobert, François de   \n",
       "bpt6k10901623$prompt-metadata.txt$openai:gpt-4o     Boisrobert, François de   \n",
       "\n",
       "                                                                                        title  \\\n",
       "response_id                                                                                     \n",
       "bpt6k10901623$prompt-excerpt.txt$ollama:mistral...  Théodore, Reyne de Hongrie, tragi-comédie   \n",
       "bpt6k10901623$prompt-summary.txt$openai:o1-mini     Théodore, Reyne de Hongrie, tragi-comédie   \n",
       "bpt6k10901623$prompt-metadata.txt$openai:gpt-4o     Théodore, Reyne de Hongrie, tragi-comédie   \n",
       "\n",
       "                                                    publication_date  \\\n",
       "response_id                                                            \n",
       "bpt6k10901623$prompt-excerpt.txt$ollama:mistral...              1658   \n",
       "bpt6k10901623$prompt-summary.txt$openai:o1-mini                 1658   \n",
       "bpt6k10901623$prompt-metadata.txt$openai:gpt-4o                 1658   \n",
       "\n",
       "                                                    document_length  \\\n",
       "response_id                                                           \n",
       "bpt6k10901623$prompt-excerpt.txt$ollama:mistral...            80779   \n",
       "bpt6k10901623$prompt-summary.txt$openai:o1-mini               80779   \n",
       "bpt6k10901623$prompt-metadata.txt$openai:gpt-4o               80779   \n",
       "\n",
       "                                                    keep_fine_tuning  \\\n",
       "response_id                                                            \n",
       "bpt6k10901623$prompt-excerpt.txt$ollama:mistral...              True   \n",
       "bpt6k10901623$prompt-summary.txt$openai:o1-mini                 True   \n",
       "bpt6k10901623$prompt-metadata.txt$openai:gpt-4o                 True   \n",
       "\n",
       "                                                      gt_period  pred_period  \\\n",
       "response_id                                                                    \n",
       "bpt6k10901623$prompt-excerpt.txt$ollama:mistral...  Middle Ages  Middle Ages   \n",
       "bpt6k10901623$prompt-summary.txt$openai:o1-mini     Middle Ages         None   \n",
       "bpt6k10901623$prompt-metadata.txt$openai:gpt-4o     Middle Ages  Middle Ages   \n",
       "\n",
       "                                                    ... gt_preferred_location  \\\n",
       "response_id                                         ...                         \n",
       "bpt6k10901623$prompt-excerpt.txt$ollama:mistral...  ...        Székesfehérvár   \n",
       "bpt6k10901623$prompt-summary.txt$openai:o1-mini     ...        Székesfehérvár   \n",
       "bpt6k10901623$prompt-metadata.txt$openai:gpt-4o     ...        Székesfehérvár   \n",
       "\n",
       "                                                                                gt_accepted_locations  \\\n",
       "response_id                                                                                             \n",
       "bpt6k10901623$prompt-excerpt.txt$ollama:mistral...  Székesfehérvár | Albe royale | Alba Regia | Hu...   \n",
       "bpt6k10901623$prompt-summary.txt$openai:o1-mini     Székesfehérvár | Albe royale | Alba Regia | Hu...   \n",
       "bpt6k10901623$prompt-metadata.txt$openai:gpt-4o     Székesfehérvár | Albe royale | Alba Regia | Hu...   \n",
       "\n",
       "                                                         pred_location  \\\n",
       "response_id                                                              \n",
       "bpt6k10901623$prompt-excerpt.txt$ollama:mistral...             Hungary   \n",
       "bpt6k10901623$prompt-summary.txt$openai:o1-mini                Hungary   \n",
       "bpt6k10901623$prompt-metadata.txt$openai:gpt-4o     Kingdom of Hungary   \n",
       "\n",
       "                                                   score_location_string  \\\n",
       "response_id                                                                \n",
       "bpt6k10901623$prompt-excerpt.txt$ollama:mistral...                  None   \n",
       "bpt6k10901623$prompt-summary.txt$openai:o1-mini                     None   \n",
       "bpt6k10901623$prompt-metadata.txt$openai:gpt-4o                     None   \n",
       "\n",
       "                                                   gt_preferred_location_QID  \\\n",
       "response_id                                                                    \n",
       "bpt6k10901623$prompt-excerpt.txt$ollama:mistral...                   Q130212   \n",
       "bpt6k10901623$prompt-summary.txt$openai:o1-mini                      Q130212   \n",
       "bpt6k10901623$prompt-metadata.txt$openai:gpt-4o                      Q130212   \n",
       "\n",
       "                                                   gt_acceptable_location_QIDs  \\\n",
       "response_id                                                                      \n",
       "bpt6k10901623$prompt-excerpt.txt$ollama:mistral...               Q130212 | Q28   \n",
       "bpt6k10901623$prompt-summary.txt$openai:o1-mini                  Q130212 | Q28   \n",
       "bpt6k10901623$prompt-metadata.txt$openai:gpt-4o                  Q130212 | Q28   \n",
       "\n",
       "                                                   pred_location_qid  \\\n",
       "response_id                                                            \n",
       "bpt6k10901623$prompt-excerpt.txt$ollama:mistral...               Q27   \n",
       "bpt6k10901623$prompt-summary.txt$openai:o1-mini                  Q28   \n",
       "bpt6k10901623$prompt-metadata.txt$openai:gpt-4o                  Q28   \n",
       "\n",
       "                                                   score_location_qid  \\\n",
       "response_id                                                             \n",
       "bpt6k10901623$prompt-excerpt.txt$ollama:mistral...               None   \n",
       "bpt6k10901623$prompt-summary.txt$openai:o1-mini                  None   \n",
       "bpt6k10901623$prompt-metadata.txt$openai:gpt-4o                  None   \n",
       "\n",
       "                                                                                   gt_location_reason  \\\n",
       "response_id                                                                                             \n",
       "bpt6k10901623$prompt-excerpt.txt$ollama:mistral...  - name of the town: Albe royale - King and que...   \n",
       "bpt6k10901623$prompt-summary.txt$openai:o1-mini     - name of the town: Albe royale - King and que...   \n",
       "bpt6k10901623$prompt-metadata.txt$openai:gpt-4o     - name of the town: Albe royale - King and que...   \n",
       "\n",
       "                                                                              pred_location_reasoning  \n",
       "response_id                                                                                            \n",
       "bpt6k10901623$prompt-excerpt.txt$ollama:mistral...  The title explicitly mentions 'Hongrie', which...  \n",
       "bpt6k10901623$prompt-summary.txt$openai:o1-mini     The title of the play refers to 'Reyne de Hong...  \n",
       "bpt6k10901623$prompt-metadata.txt$openai:gpt-4o     The title explicitly mentions 'Reyne de Hongri...  \n",
       "\n",
       "[3 rows x 26 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from textentlib.prompting import build_llm_judge_prompt\n",
    "\n",
    "def add_prompt(row) -> str:\n",
    "    \"\"\"\n",
    "    Builds a prompt for the LLM judge task based on the row data.\n",
    "    \"\"\"\n",
    "\n",
    "    prediction_columns = [col for col in df_validation_data.columns if col.startswith('pred_') and not col.endswith('reasoning')]\n",
    "    reference_columns = [col for col in df_validation_data.columns if col.startswith('gt_') and not col.endswith('reason')]\n",
    "\n",
    "    prediction_dict = row[prediction_columns].to_dict()\n",
    "    reference_dict = row[reference_columns].to_dict()\n",
    "\n",
    "    #print(reference_dict, prediction_dict)\n",
    "\n",
    "    prompt = build_llm_judge_prompt(\n",
    "        prediction=prediction_dict,\n",
    "        reference=reference_dict,\n",
    "        prompts_base_path=Path('../data/prompts/'))\n",
    "    return prompt\n",
    "\n",
    "def process_llm_judge_responses(llm_judge_responses) -> List[dict]:\n",
    "    scores  = []\n",
    "    for r in llm_judge_responses:\n",
    "        _, scores_dict = try_extract_json_from_text(r.response)\n",
    "        scores_dict['response_id'] = r.document_id\n",
    "        scores_dict['evaluator'] = r.model_name\n",
    "        scores_dict['total_tokens'] = r.total_tokens\n",
    "        scores.append(scores_dict)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation_data['llm_judge_prompt'] = df_validation_data.apply(add_prompt, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textentlib.llm_utils import LLMrequest, query_llm_judge, try_extract_json_from_text\n",
    "\n",
    "llm_judge_requests =[\n",
    "    LLMrequest(\n",
    "        prompt_id='llm_judge_prompt',\n",
    "        document_id=response_id,\n",
    "        prompt_path=None,\n",
    "        prompt=item['llm_judge_prompt']\n",
    "    )\n",
    "    for response_id, item in df_validation_data[['llm_judge_prompt']].to_dict(orient='index').items()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(llm_judge_requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ai.Client()\n",
    "client.configure({\n",
    "  \"ollama\" : {\n",
    "    \"timeout\": 600,\n",
    "  }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_judges = config['llm-judge']['models']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ollama:phi4-mini:latest',\n",
       " 'ollama:mistral-small:24b',\n",
       " 'openai:o1-mini',\n",
       " 'deepseek:deepseek-reasoner',\n",
       " 'anthropic:claude-3-7-sonnet-20250219']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_judges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping ollama:phi4-mini:latest\n",
      "Skipping ollama:mistral-small:24b\n",
      "Skipping openai:o1-mini\n",
      "Running predictions for model: deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-excerpt.txt$ollama:mistral-small:24b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-summary.txt$openai:o1-mini using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-metadata.txt$openai:gpt-4o using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-metadata.txt$deepseek:deepseek-reasoner using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-summary.txt$ollama:phi4-mini:latest using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-metadata.txt$anthropic:claude-3-7-sonnet-20250219 using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-excerpt.txt$ollama:deepseek-r1:32b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-metadata.txt$openai:o1-mini using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-summary.txt$openai:gpt-4o using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-summary.txt$ollama:deepseek-r1:32b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-metadata.txt$ollama:mistral-small:24b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-metadata.txt$ollama:phi4-mini:latest using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-excerpt.txt$ollama:deepseek-r1:14b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-summary.txt$ollama:deepseek-r1:14b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-excerpt.txt$openai:o1-mini using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-excerpt.txt$ollama:gemma3:12b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-summary.txt$ollama:gemma3:12b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-excerpt.txt$ollama:phi4-mini:latest using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-summary.txt$deepseek:deepseek-reasoner using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-excerpt.txt$deepseek:deepseek-reasoner using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-summary.txt$anthropic:claude-3-7-sonnet-20250219 using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-excerpt.txt$anthropic:claude-3-7-sonnet-20250219 using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-metadata.txt$ollama:deepseek-r1:14b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-metadata.txt$ollama:deepseek-r1:32b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-metadata.txt$ollama:gemma3:12b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-excerpt.txt$openai:gpt-4o using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-summary.txt$ollama:mistral-small:24b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-summary.txt$deepseek:deepseek-reasoner using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-summary.txt$ollama:mistral-small:24b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-excerpt.txt$deepseek:deepseek-reasoner using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-metadata.txt$ollama:gemma3:12b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-excerpt.txt$openai:o1-mini using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-excerpt.txt$ollama:phi4-mini:latest using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-summary.txt$ollama:gemma3:12b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-excerpt.txt$ollama:gemma3:12b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-metadata.txt$ollama:mistral-small:24b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-excerpt.txt$openai:gpt-4o using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-metadata.txt$openai:gpt-4o using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-metadata.txt$deepseek:deepseek-reasoner using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-metadata.txt$openai:o1-mini using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-summary.txt$openai:o1-mini using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-summary.txt$ollama:phi4-mini:latest using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-summary.txt$ollama:deepseek-r1:14b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-excerpt.txt$ollama:deepseek-r1:14b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-metadata.txt$anthropic:claude-3-7-sonnet-20250219 using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-summary.txt$ollama:deepseek-r1:32b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-excerpt.txt$ollama:deepseek-r1:32b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-excerpt.txt$anthropic:claude-3-7-sonnet-20250219 using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-summary.txt$anthropic:claude-3-7-sonnet-20250219 using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-metadata.txt$ollama:phi4-mini:latest using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-excerpt.txt$ollama:mistral-small:24b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-metadata.txt$ollama:deepseek-r1:32b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-summary.txt$openai:gpt-4o using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-metadata.txt$ollama:deepseek-r1:14b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-metadata.txt$ollama:deepseek-r1:32b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-metadata.txt$ollama:gemma3:12b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-metadata.txt$ollama:mistral-small:24b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-excerpt.txt$ollama:gemma3:12b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-metadata.txt$ollama:deepseek-r1:14b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-summary.txt$ollama:gemma3:12b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-summary.txt$anthropic:claude-3-7-sonnet-20250219 using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-summary.txt$openai:gpt-4o using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-metadata.txt$openai:o1-mini using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-summary.txt$ollama:mistral-small:24b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-excerpt.txt$anthropic:claude-3-7-sonnet-20250219 using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-metadata.txt$deepseek:deepseek-reasoner using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-summary.txt$deepseek:deepseek-reasoner using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-excerpt.txt$ollama:mistral-small:24b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-excerpt.txt$deepseek:deepseek-reasoner using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-excerpt.txt$openai:o1-mini using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-summary.txt$ollama:phi4-mini:latest using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-excerpt.txt$openai:gpt-4o using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-metadata.txt$ollama:phi4-mini:latest using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-summary.txt$openai:o1-mini using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-excerpt.txt$ollama:deepseek-r1:14b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-summary.txt$ollama:deepseek-r1:14b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-excerpt.txt$ollama:phi4-mini:latest using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-metadata.txt$openai:gpt-4o using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-metadata.txt$anthropic:claude-3-7-sonnet-20250219 using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-excerpt.txt$ollama:deepseek-r1:32b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-summary.txt$ollama:deepseek-r1:32b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-summary.txt$ollama:gemma3:12b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-metadata.txt$ollama:mistral-small:24b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-metadata.txt$ollama:phi4-mini:latest using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-excerpt.txt$ollama:gemma3:12b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-excerpt.txt$ollama:phi4-mini:latest using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-excerpt.txt$openai:gpt-4o using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-excerpt.txt$deepseek:deepseek-reasoner using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-metadata.txt$openai:o1-mini using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-summary.txt$deepseek:deepseek-reasoner using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-excerpt.txt$ollama:mistral-small:24b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-summary.txt$anthropic:claude-3-7-sonnet-20250219 using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-summary.txt$ollama:phi4-mini:latest using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-excerpt.txt$anthropic:claude-3-7-sonnet-20250219 using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-metadata.txt$openai:gpt-4o using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-summary.txt$openai:gpt-4o using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-metadata.txt$ollama:gemma3:12b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-excerpt.txt$ollama:deepseek-r1:32b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-summary.txt$ollama:deepseek-r1:32b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-summary.txt$ollama:mistral-small:24b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-summary.txt$openai:o1-mini using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-excerpt.txt$ollama:deepseek-r1:14b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-summary.txt$ollama:deepseek-r1:14b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-metadata.txt$deepseek:deepseek-reasoner using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-excerpt.txt$openai:o1-mini using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-metadata.txt$anthropic:claude-3-7-sonnet-20250219 using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-metadata.txt$ollama:deepseek-r1:32b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-metadata.txt$ollama:deepseek-r1:14b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-summary.txt$ollama:gemma3:12b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-excerpt.txt$ollama:mistral-small:24b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-metadata.txt$ollama:mistral-small:24b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-excerpt.txt$ollama:gemma3:12b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-summary.txt$openai:gpt-4o using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-metadata.txt$ollama:gemma3:12b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-metadata.txt$openai:o1-mini using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-metadata.txt$ollama:phi4-mini:latest using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-metadata.txt$anthropic:claude-3-7-sonnet-20250219 using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-metadata.txt$ollama:deepseek-r1:32b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-metadata.txt$ollama:deepseek-r1:14b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-excerpt.txt$openai:gpt-4o using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-metadata.txt$deepseek:deepseek-reasoner using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-metadata.txt$openai:gpt-4o using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-excerpt.txt$anthropic:claude-3-7-sonnet-20250219 using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-summary.txt$anthropic:claude-3-7-sonnet-20250219 using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-excerpt.txt$ollama:phi4-mini:latest using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-summary.txt$ollama:deepseek-r1:14b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-excerpt.txt$ollama:deepseek-r1:14b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-summary.txt$openai:o1-mini using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-summary.txt$ollama:deepseek-r1:32b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-excerpt.txt$ollama:deepseek-r1:32b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-excerpt.txt$openai:o1-mini using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-summary.txt$ollama:mistral-small:24b using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-excerpt.txt$deepseek:deepseek-reasoner using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-summary.txt$deepseek:deepseek-reasoner using model deepseek:deepseek-reasoner\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-summary.txt$ollama:phi4-mini:latest using model deepseek:deepseek-reasoner\n",
      "Running predictions for model: anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-excerpt.txt$ollama:mistral-small:24b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-summary.txt$openai:o1-mini using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-metadata.txt$openai:gpt-4o using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-metadata.txt$deepseek:deepseek-reasoner using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-summary.txt$ollama:phi4-mini:latest using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-metadata.txt$anthropic:claude-3-7-sonnet-20250219 using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-excerpt.txt$ollama:deepseek-r1:32b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-metadata.txt$openai:o1-mini using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-summary.txt$openai:gpt-4o using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-summary.txt$ollama:deepseek-r1:32b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-metadata.txt$ollama:mistral-small:24b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-metadata.txt$ollama:phi4-mini:latest using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-excerpt.txt$ollama:deepseek-r1:14b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-summary.txt$ollama:deepseek-r1:14b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-excerpt.txt$openai:o1-mini using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-excerpt.txt$ollama:gemma3:12b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-summary.txt$ollama:gemma3:12b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-excerpt.txt$ollama:phi4-mini:latest using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-summary.txt$deepseek:deepseek-reasoner using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-excerpt.txt$deepseek:deepseek-reasoner using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-summary.txt$anthropic:claude-3-7-sonnet-20250219 using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-excerpt.txt$anthropic:claude-3-7-sonnet-20250219 using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-metadata.txt$ollama:deepseek-r1:14b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-metadata.txt$ollama:deepseek-r1:32b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-metadata.txt$ollama:gemma3:12b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-excerpt.txt$openai:gpt-4o using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k10901623$prompt-summary.txt$ollama:mistral-small:24b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-summary.txt$deepseek:deepseek-reasoner using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-summary.txt$ollama:mistral-small:24b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-excerpt.txt$deepseek:deepseek-reasoner using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-metadata.txt$ollama:gemma3:12b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-excerpt.txt$openai:o1-mini using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-excerpt.txt$ollama:phi4-mini:latest using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-summary.txt$ollama:gemma3:12b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-excerpt.txt$ollama:gemma3:12b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-metadata.txt$ollama:mistral-small:24b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-excerpt.txt$openai:gpt-4o using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-metadata.txt$openai:gpt-4o using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-metadata.txt$deepseek:deepseek-reasoner using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-metadata.txt$openai:o1-mini using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-summary.txt$openai:o1-mini using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-summary.txt$ollama:phi4-mini:latest using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-summary.txt$ollama:deepseek-r1:14b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-excerpt.txt$ollama:deepseek-r1:14b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-metadata.txt$anthropic:claude-3-7-sonnet-20250219 using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-summary.txt$ollama:deepseek-r1:32b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-excerpt.txt$ollama:deepseek-r1:32b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-excerpt.txt$anthropic:claude-3-7-sonnet-20250219 using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-summary.txt$anthropic:claude-3-7-sonnet-20250219 using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-metadata.txt$ollama:phi4-mini:latest using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-excerpt.txt$ollama:mistral-small:24b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-metadata.txt$ollama:deepseek-r1:32b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-summary.txt$openai:gpt-4o using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k1090242p$prompt-metadata.txt$ollama:deepseek-r1:14b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-metadata.txt$ollama:deepseek-r1:32b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-metadata.txt$ollama:gemma3:12b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-metadata.txt$ollama:mistral-small:24b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-excerpt.txt$ollama:gemma3:12b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-metadata.txt$ollama:deepseek-r1:14b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-summary.txt$ollama:gemma3:12b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-summary.txt$anthropic:claude-3-7-sonnet-20250219 using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-summary.txt$openai:gpt-4o using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-metadata.txt$openai:o1-mini using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-summary.txt$ollama:mistral-small:24b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-excerpt.txt$anthropic:claude-3-7-sonnet-20250219 using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-metadata.txt$deepseek:deepseek-reasoner using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-summary.txt$deepseek:deepseek-reasoner using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-excerpt.txt$ollama:mistral-small:24b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-excerpt.txt$deepseek:deepseek-reasoner using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-excerpt.txt$openai:o1-mini using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-summary.txt$ollama:phi4-mini:latest using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-excerpt.txt$openai:gpt-4o using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-metadata.txt$ollama:phi4-mini:latest using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-summary.txt$openai:o1-mini using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-excerpt.txt$ollama:deepseek-r1:14b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-summary.txt$ollama:deepseek-r1:14b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-excerpt.txt$ollama:phi4-mini:latest using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-metadata.txt$openai:gpt-4o using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-metadata.txt$anthropic:claude-3-7-sonnet-20250219 using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-excerpt.txt$ollama:deepseek-r1:32b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k852913n$prompt-summary.txt$ollama:deepseek-r1:32b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-summary.txt$ollama:gemma3:12b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-metadata.txt$ollama:mistral-small:24b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-metadata.txt$ollama:phi4-mini:latest using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-excerpt.txt$ollama:gemma3:12b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-excerpt.txt$ollama:phi4-mini:latest using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-excerpt.txt$openai:gpt-4o using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-excerpt.txt$deepseek:deepseek-reasoner using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-metadata.txt$openai:o1-mini using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-summary.txt$deepseek:deepseek-reasoner using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-excerpt.txt$ollama:mistral-small:24b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-summary.txt$anthropic:claude-3-7-sonnet-20250219 using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-summary.txt$ollama:phi4-mini:latest using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-excerpt.txt$anthropic:claude-3-7-sonnet-20250219 using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-metadata.txt$openai:gpt-4o using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-summary.txt$openai:gpt-4o using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-metadata.txt$ollama:gemma3:12b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-excerpt.txt$ollama:deepseek-r1:32b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-summary.txt$ollama:deepseek-r1:32b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-summary.txt$ollama:mistral-small:24b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-summary.txt$openai:o1-mini using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-excerpt.txt$ollama:deepseek-r1:14b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-summary.txt$ollama:deepseek-r1:14b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-metadata.txt$deepseek:deepseek-reasoner using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-excerpt.txt$openai:o1-mini using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-metadata.txt$anthropic:claude-3-7-sonnet-20250219 using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-metadata.txt$ollama:deepseek-r1:32b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k5772699f$prompt-metadata.txt$ollama:deepseek-r1:14b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-summary.txt$ollama:gemma3:12b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-excerpt.txt$ollama:mistral-small:24b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-metadata.txt$ollama:mistral-small:24b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-excerpt.txt$ollama:gemma3:12b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-summary.txt$openai:gpt-4o using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-metadata.txt$ollama:gemma3:12b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-metadata.txt$openai:o1-mini using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-metadata.txt$ollama:phi4-mini:latest using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-metadata.txt$anthropic:claude-3-7-sonnet-20250219 using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-metadata.txt$ollama:deepseek-r1:32b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-metadata.txt$ollama:deepseek-r1:14b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-excerpt.txt$openai:gpt-4o using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-metadata.txt$deepseek:deepseek-reasoner using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-metadata.txt$openai:gpt-4o using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-excerpt.txt$anthropic:claude-3-7-sonnet-20250219 using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-summary.txt$anthropic:claude-3-7-sonnet-20250219 using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-excerpt.txt$ollama:phi4-mini:latest using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-summary.txt$ollama:deepseek-r1:14b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-excerpt.txt$ollama:deepseek-r1:14b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-summary.txt$openai:o1-mini using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-summary.txt$ollama:deepseek-r1:32b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-excerpt.txt$ollama:deepseek-r1:32b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-excerpt.txt$openai:o1-mini using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-summary.txt$ollama:mistral-small:24b using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-excerpt.txt$deepseek:deepseek-reasoner using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-summary.txt$deepseek:deepseek-reasoner using model anthropic:claude-3-7-sonnet-20250219\n",
      "Processing prompt llm_judge_prompt for document bpt6k9807756q$prompt-summary.txt$ollama:phi4-mini:latest using model anthropic:claude-3-7-sonnet-20250219\n"
     ]
    }
   ],
   "source": [
    "# Query LLMs and collect responses\n",
    "reasoning_llms = ['openai:o1-mini', 'deepseek:deepseek-reasoner', 'anthropic:claude-3-7-sonnet-20250219']\n",
    "default_temperature = config['llm-judge']['temperature']\n",
    "scores_output_path = Path(config['llm-judge']['scores_output_path'])\n",
    "\n",
    "for model in llm_judges:\n",
    "    # ask only LLMs with an API\n",
    "    if model.startswith('ollama') or model == \"openai:o1-mini\":\n",
    "        print(f'Skipping {model}')\n",
    "        continue\n",
    "\n",
    "    # Query the candidate judge LLMs\n",
    "    llm_judge_responses = []\n",
    "    print('Running predictions for model:', model)\n",
    "    if model in reasoning_llms:\n",
    "        llm_judge_responses += query_llm_judge(client, model, llm_judge_requests)\n",
    "    else:\n",
    "        llm_judge_responses += query_llm_judge(client, model, llm_judge_requests, temperature=default_temperature)\n",
    "    \n",
    "    # Stores produced socres in a TSV file\n",
    "    scores  = process_llm_judge_responses(llm_judge_responses)\n",
    "    scores_tsv_path = base_path / scores_output_path / f\"{model}_scores.tsv\"\n",
    "    pd.DataFrame(scores).to_csv(f\"{scores_tsv_path}\", sep='\\t')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute IAA between scorers\n",
    "\n",
    "- read in all the scores on the validation docs (human + LLM-judge)\n",
    "- reshape dataframe so to have for each document and for each score type, all the scores assigned\n",
    "- for each score type compute the IAA\n",
    "- compute an average IAA across all score types"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
