{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aisuite as ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ai.Client()\n",
    "client.configure({\n",
    "  \"ollama\" : {\n",
    "    \"timeout\": 600,\n",
    "  }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test `aisuite` with dummy prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Respond in Pirate English. Always try to include the phrase - No rum No fun.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a joke about Captain Jack Sparrow\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME               ID              SIZE      MODIFIED          \n",
      "phi4:latest        ac896e5b8b34    9.1 GB    55 seconds ago       \n",
      "gemma2:9b          ff02c3702f32    5.4 GB    41 minutes ago       \n",
      "llama3.2:latest    a80c4f17acd5    2.0 GB    About an hour ago    \n",
      "deepseek-r1:8b     28f8fd6cdc67    4.9 GB    2 hours ago          \n",
      "llama3.3:latest    a6eb4748fd29    42 GB     3 weeks ago          \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    #\"ollama:deepseek-r1:8b\",\n",
    "    #\"ollama:llama:3.3:latest\",\n",
    "    \"ollama:llama3.2:latest\",\n",
    "    \"ollama:gemma2:9b\",\n",
    "    \"ollama:phi4:latest\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies = {}\n",
    "\n",
    "for selected_model in models:\n",
    "    response = client.chat.completions.create(model=selected_model, messages=messages)\n",
    "    replies[selected_model] = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ollama:llama3.2:latest; reply's length = 243\n",
      "Model: ollama:gemma2:9b; reply's length = 304\n"
     ]
    }
   ],
   "source": [
    "for k,v in replies.items():\n",
    "    print(f\"Model: {k}; reply's length = {len(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ollama:llama3.2:latest': \"Yer lookin' fer a joke about that scurvy dog, eh? Alright then, listen close:\\n\\nWhy did Captain Jack Sparrow bring a ladder aboard his ship?\\n\\nBecause he heard the drinks were on the house! Arrr, no rum, no fun!\",\n",
       " 'ollama:gemma2:9b': \"Ahoy, matey! Ye want a tale 'bout ol' Jack Sparrow? \\n\\nGather 'round and listen close:\\n\\nWhy did Captain Jack Sparrow always carry two compasses? \\n\\nTo be sure he wasn't lost at sea...and to have one to point the way to the nearest grog stash! No rum, no fun, ye hear?  üçªüíÄ\\n\\n\\n\",\n",
       " 'ollama:phi4:latest': 'Ahoy there, matey! Gather \\'round for a tale o\\' ol\\' Cap\\'n Jack Sparrow!\\n\\nSo, what happens when you mix Captain Jack Sparrow with a chicken?\\n\\nYou get... \"Cluckin\\' up the wrong ship!\"\\n\\nArrr, no rum, no fun! But remember, ye never know where yer adventure will take ye next!'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query LLMs with real TextEnt data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for each document, load the pre-generated summary\n",
    "- based on the summary, for each doc generate 3 prompts (metadata, metadata + incipit, metadata + summary)\n",
    "- iterate over doc, iterate over prompts per doc, iterate over models, then query with triples (docu, model, prompt)\n",
    "\n",
    "- start with a spacy document\n",
    "- load the corresponding pre-generated summary\n",
    "- define a `build_prompts` function that takes a `spacy_doc` as input and returns a list of tuples `('prompt-id', 'prompt-message')` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "def build_summary_prompt(spacy_doc: Doc) -> str:\n",
    "    \"\"\"\n",
    "    Builds a summary prompt based on a spaCy document.\n",
    "\n",
    "    Args:\n",
    "        spacy_doc (Doc): A spaCy document object containing the text and metadata.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted summary prompt.\n",
    "\n",
    "    The summary is loaded from a JSON file located in the \"../data/summaries\" directory.\n",
    "    The filename of the summary is derived from the 'document_id' stored in the user_data attribute of the spaCy document.\n",
    "    \"\"\"\n",
    "\n",
    "    # load base prompt\n",
    "    with open(\"../data/prompts/summary_prompt.txt\", \"r\") as file:\n",
    "        base_prompt = file.read()\n",
    "\n",
    "    # load the pre-computed summary from its JSON file\n",
    "    summaries_path = Path(\"../data/summaries\")\n",
    "    doc_summary_path = summaries_path / f\"{spacy_doc.user_data['document_id']}_summary.json\"\n",
    "\n",
    "    try:    \n",
    "        with doc_summary_path.open('r', encoding='utf-8') as file:\n",
    "            summary = json.load(file)\n",
    "    except:\n",
    "        #print(f'No summary for document {spacy_doc.user_data[\"document_id\"]}')\n",
    "        return None\n",
    "\n",
    "    # JSON to pretty string\n",
    "    summary_as_string = json.dumps(summary, indent=2, ensure_ascii=False)\n",
    "    return base_prompt.format(document_summary=summary_as_string)\n",
    "\n",
    "\n",
    "def build_excerpt_prompt(spacy_doc: Doc, excerpt_length: int = 400) -> str:\n",
    "    summaries_path = Path(\"../data/summaries\")\n",
    "    doc_summary_path = summaries_path / f\"{spacy_doc.user_data['document_id']}_summary.json\"\n",
    "\n",
    "    # load excerpt prompt\n",
    "    with open(\"../data/prompts/excerpt_prompt.txt\", \"r\") as file:\n",
    "        prompt = file.read()\n",
    "\n",
    "    # load the pre-computed summary from its JSON file\n",
    "    summaries_path = Path(\"../data/summaries\")\n",
    "    doc_summary_path = summaries_path / f\"{spacy_doc.user_data['document_id']}_summary.json\"\n",
    "    \n",
    "    try:    \n",
    "        with doc_summary_path.open('r', encoding='utf-8') as file:\n",
    "            summary = json.load(file)\n",
    "    except:\n",
    "        #print(f'No summary for document {spacy_doc.user_data[\"document_id\"]}')\n",
    "        return None\n",
    "\n",
    "    text_length = len(spacy_doc.text)\n",
    "    mid_point = text_length // 2\n",
    "    left_boundary = mid_point - (excerpt_length // 2) \n",
    "    right_boundary = mid_point + (excerpt_length // 2)\n",
    "    excerpt = spacy_doc.text[left_boundary:right_boundary]\n",
    "\n",
    "    # JSON to pretty string\n",
    "    json_doc = {\n",
    "        'metadata': summary['metadata'],\n",
    "        'excerpt': excerpt\n",
    "    }\n",
    "    json_doc_as_string = json.dumps(json_doc, indent=2, ensure_ascii=False)\n",
    "    return prompt.format(document=json_doc_as_string, excerpt_length=excerpt_length)\n",
    "\n",
    "def build_metadata_prompt(spacy_doc: Doc) -> str:\n",
    "    summaries_path = Path(\"../data/summaries\")\n",
    "    doc_summary_path = summaries_path / f\"{spacy_doc.user_data['document_id']}_summary.json\"\n",
    "\n",
    "    # load metadata prompt\n",
    "    with open(\"../data/prompts/metadata_prompt.txt\", \"r\") as file:\n",
    "        metadata_prompt = file.read()\n",
    "\n",
    "    # load the pre-computed summary from its JSON file\n",
    "    summaries_path = Path(\"../data/summaries\")\n",
    "    doc_summary_path = summaries_path / f\"{spacy_doc.user_data['document_id']}_summary.json\"\n",
    "    \n",
    "    try:    \n",
    "        with doc_summary_path.open('r', encoding='utf-8') as file:\n",
    "            summary = json.load(file)\n",
    "    except:\n",
    "        #print(f'No summary for document {spacy_doc.user_data[\"document_id\"]}')\n",
    "        return None\n",
    "\n",
    "    # JSON to pretty string\n",
    "    metadata = {'metadata': summary['metadata']}\n",
    "    metadata_as_string = json.dumps(metadata, indent=2, ensure_ascii=False)\n",
    "    return metadata_prompt.format(document_metadata=metadata_as_string)\n",
    "\n",
    "def build_prompts(spacy_doc: Doc) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Builds prompts based on a spaCy document.\n",
    "\n",
    "    Args:\n",
    "        spacy_doc (Doc): A spaCy document object containing the text and metadata.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, str]]: A list of tuples where each tuple contains a prompt ID and its text.    \n",
    "    \"\"\"  \n",
    "    return [\n",
    "        ('prompt-summary', build_summary_prompt(spacy_doc)),\n",
    "        ('prompt-metadata', build_metadata_prompt(spacy_doc)),\n",
    "        ('prompt-excerpt', build_excerpt_prompt(spacy_doc)),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_generate_prompts(spacy_docs: List[Doc], output_path: Path) -> None:\n",
    "\n",
    "    problems = []\n",
    "\n",
    "    for spacy_doc in tqdm(spacy_docs, desc=\"Pre-generating prompts\"):\n",
    "        doc_id = spacy_doc.user_data[\"document_id\"]\n",
    "        prompts = build_prompts(spacy_doc)\n",
    "\n",
    "        # Define the path to the directory\n",
    "        directory_path = output_path / doc_id\n",
    "\n",
    "        # Check if the directory exists\n",
    "        if not directory_path.exists():\n",
    "            directory_path.mkdir(parents=True, exist_ok=True) # Create the directory if it does not exist\n",
    "\n",
    "        for prompt_id, prompt in prompts:\n",
    "            if prompt:\n",
    "                #print(f\"Writing prompt {prompt_id} for document {doc_id}\")\n",
    "                with open(output_path / doc_id / f\"{doc_id}_{prompt_id}.txt\", \"w\") as file:\n",
    "                    file.write(prompt)\n",
    "            else:\n",
    "                problems.append(f'There was a problem with generating prompt {prompt_id} for document {doc_id}')\n",
    "    \n",
    "    print(\"\\n\".join(problems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "from textentlib.utils import load_or_create_corpus, nlp_model_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACY_CORPUS_SERIALIZED_PATH = Path(\"../data/corpus_24022025.spacy\")\n",
    "PRE_GENERATED_PROMPTS_PATH = Path(\"../data/prompts/pregenerated\")    \n",
    "SAMPLE_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded serialize spacy corpus from ../data/corpus_24022025.spacy\n",
      "Number of documents in the corpus: 594\n",
      "Number of entities in the corpus: 287389\n",
      "Number of tokens in the corpus: 12885306\n"
     ]
    }
   ],
   "source": [
    "spacy_corpus = load_or_create_corpus(SPACY_CORPUS_SERIALIZED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = spacy_corpus.get_docs(nlp_model_fr.vocab)\n",
    "docs = list(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - we may want to exclude documents in the validation set\n",
    "# - we may want to exclude documents that are very long (> 150k tokens)\n",
    "sampled_docs = random.sample(docs, SAMPLE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sampled_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look at the following JSON object describing a theatre play in French (XVII century); the `metadata` property contains basic information about the play (author, title, publication date), while the `excerpt` property contains an excerpt of 400 words sampled from around the middle of the document.\n",
      "\n",
      "INPUT:\n",
      "```json\n",
      "{\n",
      "  \"metadata\": {\n",
      "    \"author\": \"Marcassus, Pierre de\",\n",
      "    \"title\": \"Les pescheurs illustres\",\n",
      "    \"publication_date\": \"1648\",\n",
      "    \"document_id\": \"bpt6k5811892h\"\n",
      "  },\n",
      "  \"excerpt\": \"n nouveau danger. √Ä peine ai-je le bien de la pouvoir entendre, Que j'entends quelle crie: √¥ trop fid√®le Alcandre O√π vas-tu donc? h√© quoi? me miens-tu secourir Gagne gagne la rive; et me laisse p√©rir. Quels exc√®s de bont√© derechef te retardent? Je ne m√©rite plus que tes yeux me regardent. Charm√© du faux √©clat d'une langage si doux Contre les flots mutins ie redouble mes cous: Repousse les assaus d\"\n",
      "}\n",
      "```\n",
      "\n",
      "Your role is to predict the location and historical period in which the action of the play is set. \n",
      "\n",
      "KEY RULES:\n",
      "- Predict the timespan and not the precise and exact date of the period where the play could have taken place\n",
      "- Do not write an introduction or summary \n",
      "- The response must contain only valid JSON\n",
      "- The values in the JSON \"timeframe_start\" and \"timeframe_end\" should always be a single valid ISO date in the form YYYY-MM-DD\n",
      "- if the provided information is not sufficient to determine historical period and/or location, the following values can be set to `None`: `period`, `timeframe_start`, `timeframe_end`, `location`, `location_qid`\n",
      "\n",
      "Return your response and the underlying reasoning as a JSON object with the following structure:\n",
      "```json\n",
      "{\n",
      "    \"period\": \"The historical period in which the play could have taken place\",\n",
      "    \"period_reasoning\": \"The reasoning the model used to identify the historical period\",\n",
      "    \"timeframe_start\": \"The ISO-formatted start value of the historical period\",\n",
      "    \"timeframe_end\": \"The ISO-formatted start value of the historical period\",\n",
      "    \"location\": \"The geographic location where the action of the play takes place\",\n",
      "    \"location_reasoning\": \"The reasoning the model used to identify the geographic location\",\n",
      "    \"location_qid\": \"The Wikidata QID of the identified location\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(build_excerpt_prompt(sampled_docs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sampled_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-generating prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:01<00:00, 42.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was a problem with generating prompt prompt-summary for document bpt6k1522463f\n",
      "There was a problem with generating prompt prompt-metadata for document bpt6k1522463f\n",
      "There was a problem with generating prompt prompt-excerpt for document bpt6k1522463f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pre_generate_prompts(sampled_docs, PRE_GENERATED_PROMPTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class LLMrequest:\n",
    "    prompt_id: str\n",
    "    document_id: str\n",
    "    prompt_path: Path\n",
    "    prompt: str\n",
    "\n",
    "@dataclass\n",
    "class LLMresponse:\n",
    "    document_id: str\n",
    "    prompt_id: str\n",
    "    prompt: str\n",
    "    model_name: str\n",
    "    response: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aisuite as ai\n",
    "\n",
    "client = ai.Client()\n",
    "client.configure({\n",
    "  \"ollama\" : {\n",
    "    \"timeout\": 600,\n",
    "  }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm(model: str, requests: List[LLMrequest]) -> List[LLMresponse]:\n",
    "    # pass over the requests to a given model and gather the responses\n",
    "    responses = []\n",
    "    for request in requests:\n",
    "        # TODO: avoid asking the model if an answer file already exists\n",
    "        print(f\"Processing prompt {request.prompt_id} for document {request.document_id} using model {model}\")\n",
    "        response = client.chat.completions.create(model=model, messages=[{\"role\": \"system\", \"content\": request.prompt}])\n",
    "        llm_response = LLMresponse(\n",
    "            document_id=request.document_id,\n",
    "            prompt_id=request.prompt_id,\n",
    "            prompt=request.prompt,\n",
    "            model_name=model,\n",
    "            response=response.choices[0].message.content\n",
    "        )\n",
    "        responses.append(llm_response)\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_llm_responses(responses: List[LLMresponse], output_path: Path) -> None:\n",
    "\n",
    "    for response in responses:\n",
    "\n",
    "        output_dir = Path(output_path / response.document_id)\n",
    "\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        filename = f\"{response.document_id}_{response.prompt_id}_{response.model_name.replace(':', '-')}.txt\"\n",
    "        filepath = output_path / response.document_id / filename\n",
    "        response_trimmed = response.response.replace('```json', '').replace('```', '').strip()\n",
    "\n",
    "        with filepath.open(\"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(response_trimmed)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_requests = []\n",
    "\n",
    "for subdir in Path('../data/prompts/pregenerated').iterdir():\n",
    "    for file in subdir.iterdir():\n",
    "        doc_id, prompt_id = file.name.split('_')\n",
    "        prompt_id = prompt_id.split('.')[0]\n",
    "        prompt = file.read_text()\n",
    "        llm_requests.append(LLMrequest(prompt_id, doc_id, file, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prompt prompt-summary for document bpt6k15110748 using model ollama:llama3.2:latest\n",
      "Processing prompt prompt-metadata for document bpt6k15110748 using model ollama:llama3.2:latest\n",
      "Processing prompt prompt-excerpt for document bpt6k15110748 using model ollama:llama3.2:latest\n",
      "Processing prompt prompt-metadata for document bpt6k8569801 using model ollama:llama3.2:latest\n",
      "Processing prompt prompt-excerpt for document bpt6k8569801 using model ollama:llama3.2:latest\n",
      "Processing prompt prompt-summary for document bpt6k8569801 using model ollama:llama3.2:latest\n",
      "Processing prompt prompt-excerpt for document btv1b8622118r using model ollama:llama3.2:latest\n",
      "Processing prompt prompt-metadata for document btv1b8622118r using model ollama:llama3.2:latest\n",
      "Processing prompt prompt-summary for document btv1b8622118r using model ollama:llama3.2:latest\n",
      "Processing prompt prompt-summary for document bpt6k15110748 using model ollama:gemma2:9b\n",
      "Processing prompt prompt-metadata for document bpt6k15110748 using model ollama:gemma2:9b\n",
      "Processing prompt prompt-excerpt for document bpt6k15110748 using model ollama:gemma2:9b\n",
      "Processing prompt prompt-metadata for document bpt6k8569801 using model ollama:gemma2:9b\n",
      "Processing prompt prompt-excerpt for document bpt6k8569801 using model ollama:gemma2:9b\n"
     ]
    }
   ],
   "source": [
    "llm_responses = []\n",
    "\n",
    "models = [\n",
    "    #\"ollama:deepseek-r1:8b\",\n",
    "    #\"ollama:llama:3.3:latest\",\n",
    "    \"ollama:llama3.2:latest\",\n",
    "    \"ollama:gemma2:9b\",\n",
    "    \"ollama:phi4:latest\"\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    llm_responses += query_llm(model, llm_requests[:9])\n",
    "\n",
    "serialize_llm_responses(llm_responses, Path('../data/llm_responses'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ollama:gemma2:9b; Prompt: Look at the following JSON object describing a theatre play in French (XVII century); the `metadata` property contains basic information about the play (author, title, publication date), while the `context` property contains information about the people and places that are most frequently mentioned in the play (such as label, mention frequency, and salient sentences where it appears).\n",
      "\n",
      "INPUT:\n",
      "```json\n",
      "{\n",
      "  \"metadata\": {\n",
      "    \"author\": \"Tristan L'Hermite\",\n",
      "    \"title\": \"Panth√©e, trag√©die de M. de Tristan\",\n",
      "    \"publication_date\": \"1639\",\n",
      "    \"document_id\": \"bpt6k15110748\"\n",
      "  },\n",
      "  \"context\": {\n",
      "    \"people\": {\n",
      "      \"top_1_person\": {\n",
      "        \"entity\": {\n",
      "          \"label\": \"CYRUS\",\n",
      "          \"frequency\": 3\n",
      "        },\n",
      "        \"related_sentences\": [\n",
      "          \"PANTH√âE\\n Seigneur, votre bont√© s'est acquise Abradate J'ai d√©p√™ch√© des miens pour lui faire savoir Qu'elles sont vos vertus, et quel est son devoir: S'il n'a chang√© d'esprit j'ose bien me promettre Qu'il viendra vous trouver ayant re√ßu ma lettre, Et qu'il vous servira s'il est autant heureux en alitours qre paeut et gencren: \\nCYRUS\\n Madame, notre n'ai point m√©rit√© cette gr√¢ce.\",\n",
      "          \"Charis apprend de lui sa passion, essaie de le gu√©rir par la raison: et le plaint voyant sa maladie incurable. \\nPANTH√âE\\n EITI AE TRIRIEDIE ICTEFREMI√àR SC√àNE IREMI√àRE. \\nCYRUS\\n \\nCHRISANTE\\n HYDASPE. \\nCYRUS\\n HRISANTE, ils sont d√©faits, et c'est notre destin De revenir charg√©s d'honneur et de butin, Apres avoir dompt√© cette injuste Puissance Qui veut insolemment opprimer l'innocence.\",\n",
      "          \"JRUS\\n Cet acte, sans mentir, fait horreur et piti√©, Un moindre coup peut rompre une grande amiti√©: J'ai su d'vn Gouverneur d'une de ces Provinces, Qu'on le pourrait compter entre les mauvais Princes. \\nPANTH√âE\\n It me aome √† peu pres que est ec Gounerneur \\nCYRUS\\n C'est un p√®re afflig√©: \\nPANTH√âE\\n \",\n",
      "          \"Carpuisqu'on doit aimer enomeun prus quej√† cte, \\nCYRUS\\n Madame, les honneurs dont il me va chargeant Font voir en ce papier qu'il est fort obligeant, Par ses grands compliments il ma voulu confondre, √Ä ces civilit√©s apr√®s ne saurais r√©pondre: Mais si pour le servir ainsi suis assez puissant, Il ne me tiendra pas pour un m√©connaissant:\",\n",
      "          \"Sans doute votre esprit qui n'a point de d√©faut, Le loue avec exc√®s, en le mettant si haut. \\nPANTH√âE\\n Cyrus m'a fait faveur, mais ainsi lui rends iustice, Quand j'atteste qu'il est inaccesible au vice, Et qu'on peut l'√©lever entre les immortels, Onis grareros certus mermem u Tels:\"\n",
      "        ]\n",
      "      },\n",
      "      \"top_5_persons\": [\n",
      "        \"CYRUS\",\n",
      "        \"Cirus\",\n",
      "        \"Cyrus\",\n",
      "        \"Seigneur\",\n",
      "        \"Charis\"\n",
      "      ]\n",
      "    },\n",
      "    \"places\": {\n",
      "      \"top_1_place\": {\n",
      "        \"entity\": {\n",
      "          \"label\": \"Paris\",\n",
      "          \"frequency\": 3\n",
      "        },\n",
      "        \"related_sentences\": [\n",
      "          \"Notre bien am√© Augustin Courb√© Libraire √† Paris, nous a fait remontrer qu'il d√©sirerait imprimer, Une Trag√©die intitul√©e, Panth√©e, compos√©e par le Sieur de Tristanl'Hermite, S'il avait sur ce nos Lettres n√©cessaires, lesquelles ils nous a tr√®s humblement suppli√© de leurs accorder: √Ä CES CAUSES, Nous avons permis et permettons √† l'exposant d'imprimer, vendre, et d√©biter en tous les lieux de notre ob√©issance ladite Trag√©die, en telles marges, en tels caract√®res, etautant de fois qu'il voudra, durant l'espace de sept ans entiers et accomplis;\",\n",
      "          \"Donn√© √† Paris le vingt troisi√®me de F√©vrier l'an de gr√¢ce mil six cents trente huit Et de notre r√®gne le vingt-huicti√™me.\",\n",
      "          \"Donn√© √† Paris le vingt troisi√®me de F√©vrier l'an de gr√¢ce mil six cents trente huit Et de notre r√®gne le vingt-huicti√™me.\"\n",
      "        ]\n",
      "      },\n",
      "      \"top_5_places\": [\n",
      "        \"Paris\",\n",
      "        \"Babylone\",\n",
      "        \"Perse\",\n",
      "        \"Th√©√¢tre du Marais\",\n",
      "        \"France\"\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "Your role is to predict the location and historical period in which the action of the play is set. \n",
      "\n",
      "KEY RULES:\n",
      "- Predict the timespan and not the precise and exact date of the period where the play could have taken place\n",
      "- Do not write an introduction or summary \n",
      "- The response must contain only valid JSON\n",
      "- The values in the JSON \"timeframe_start\" and \"timeframe_end\" should always be a single valid ISO date in the form YYYY-MM-DD\n",
      "- if the provided information is not sufficient to determine historical period and/or location, the following values can be set to `None`: `period`, `timeframe_start`, `timeframe_end`, `location`, `location_qid`\n",
      "\n",
      "Return your response and the underlying reasoning as a JSON object with the following structure:\n",
      "```json\n",
      "{\n",
      "    \"period\": \"The historical period in which the play could have taken place\",\n",
      "    \"period_reasoning\": \"The reasoning the model used to identify the historical period\",\n",
      "    \"timeframe_start\": \"The ISO-formatted start value of the historical period\",\n",
      "    \"timeframe_end\": \"The ISO-formatted start value of the historical period\",\n",
      "    \"location\": \"The geographic location where the action of the play takes place\",\n",
      "    \"location_reasoning\": \"The reasoning the model used to identify the geographic location\",\n",
      "    \"location_qid\": \"The Wikidata QID of the identified location\"\n",
      "}\n",
      "```\n",
      "Response: ```json\n",
      "{\n",
      "    \"period\": \"Ancient Persia/Classical Antiquity\",\n",
      "    \"period_reasoning\": \"Names like Cyrus and characters possibly referencing Babylonian or Persian culture suggest a setting in ancient Persia.\",\n",
      "    \"timeframe_start\": \"330-01-01\",\n",
      "    \"timeframe_end\": \"550-12-31\",\n",
      "    \"location\": \"Persia/Babylonian Empire\",\n",
      "    \"location_reasoning\": \"The mention of Cyrus, a prominent Persian king, and potential references to Babylon or Persia strongly point towards this region.\",\n",
      "    \"location_qid\": \"Q984\"\n",
      "}\n",
      "```\n",
      "Model: ollama:gemma2:9b; Prompt: Look at the following JSON object describing a theatre play in French (XVII century); the `metadata` property contains basic information about the play (author, title, publication date).\n",
      "\n",
      "INPUT:\n",
      "```json\n",
      "{\n",
      "  \"metadata\": {\n",
      "    \"author\": \"Tristan L'Hermite\",\n",
      "    \"title\": \"Panth√©e, trag√©die de M. de Tristan\",\n",
      "    \"publication_date\": \"1639\",\n",
      "    \"document_id\": \"bpt6k15110748\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "Your role is to predict the location and historical period in which the action of the play is set. \n",
      "\n",
      "KEY RULES:\n",
      "- Predict the timespan and not the precise and exact date of the period where the play could have taken place\n",
      "- Do not write an introduction or summary \n",
      "- The response must contain only valid JSON\n",
      "- The values in the JSON \"timeframe_start\" and \"timeframe_end\" should always be a single valid ISO date in the form YYYY-MM-DD\n",
      "- if the provided information is not sufficient to determine historical period and/or location, the following values can be set to `None`: `period`, `timeframe_start`, `timeframe_end`, `location`, `location_qid`\n",
      "\n",
      "Return your response and the underlying reasoning as a JSON object with the following structure:\n",
      "```json\n",
      "{\n",
      "    \"period\": \"The historical period in which the play could have taken place\",\n",
      "    \"period_reasoning\": \"The reasoning the model used to identify the historical period\",\n",
      "    \"timeframe_start\": \"The ISO-formatted start value of the historical period\",\n",
      "    \"timeframe_end\": \"The ISO-formatted start value of the historical period\",\n",
      "    \"location\": \"The geographic location where the action of the play takes place\",\n",
      "    \"location_reasoning\": \"The reasoning the model used to identify the geographic location\",\n",
      "    \"location_qid\": \"The Wikidata QID of the identified location\"\n",
      "}\n",
      "```\n",
      "Response: ```json\n",
      "{\n",
      "    \"period\": \"Classical Antiquity\",\n",
      "    \"period_reasoning\": \"The title \\\"Panth√©e, trag√©die de M. de Tristan\\\"  suggests a play based on Greek mythology, as Panth√©e is a figure from Classical Antiquity.\",\n",
      "    \"timeframe_start\": \"1000-01-01\",\n",
      "    \"timeframe_end\": \"500-12-31\",\n",
      "    \"location\": \"Ancient Greece\",\n",
      "    \"location_reasoning\": \"The play's title referencing Panth√©e, a figure from Greek mythology, strongly suggests the action takes place in Ancient Greece.\",\n",
      "    \"location_qid\": \"Q47985\" \n",
      "}\n",
      "```\n",
      "Model: ollama:gemma2:9b; Prompt: Look at the following JSON object describing a theatre play in French (XVII century); the `metadata` property contains basic information about the play (author, title, publication date), while the `excerpt` property contains an excerpt of 400 words sampled from around the middle of the document.\n",
      "\n",
      "INPUT:\n",
      "```json\n",
      "{\n",
      "  \"metadata\": {\n",
      "    \"author\": \"Tristan L'Hermite\",\n",
      "    \"title\": \"Panth√©e, trag√©die de M. de Tristan\",\n",
      "    \"publication_date\": \"1639\",\n",
      "    \"document_id\": \"bpt6k15110748\"\n",
      "  },\n",
      "  \"excerpt\": \"u'on n'a point vu qu'en expiant mon crime T'offrisse √† son Autel une indigne Victime, Et que voyant venir le coup de mon tr√©pas, Je n'ai rien t√©moign√© de faible ainsi de bas. Despein-lui ma constance, et la mets en son lustre; Jure avait que mon √¢me, √©tait une √¢me illustre, Et que d√®s le moment que j'entrai sous sa loi, J'eus l'esprit, le courage, et la grandeur d'un Roi. Elle n'est point de marbr\"\n",
      "}\n",
      "```\n",
      "\n",
      "Your role is to predict the location and historical period in which the action of the play is set. \n",
      "\n",
      "KEY RULES:\n",
      "- Predict the timespan and not the precise and exact date of the period where the play could have taken place\n",
      "- Do not write an introduction or summary \n",
      "- The response must contain only valid JSON\n",
      "- The values in the JSON \"timeframe_start\" and \"timeframe_end\" should always be a single valid ISO date in the form YYYY-MM-DD\n",
      "- if the provided information is not sufficient to determine historical period and/or location, the following values can be set to `None`: `period`, `timeframe_start`, `timeframe_end`, `location`, `location_qid`\n",
      "\n",
      "Return your response and the underlying reasoning as a JSON object with the following structure:\n",
      "```json\n",
      "{\n",
      "    \"period\": \"The historical period in which the play could have taken place\",\n",
      "    \"period_reasoning\": \"The reasoning the model used to identify the historical period\",\n",
      "    \"timeframe_start\": \"The ISO-formatted start value of the historical period\",\n",
      "    \"timeframe_end\": \"The ISO-formatted start value of the historical period\",\n",
      "    \"location\": \"The geographic location where the action of the play takes place\",\n",
      "    \"location_reasoning\": \"The reasoning the model used to identify the geographic location\",\n",
      "    \"location_qid\": \"The Wikidata QID of the identified location\"\n",
      "}\n",
      "```\n",
      "Response: ```json\n",
      "{\n",
      "    \"period\": \"Early Modern Period\",\n",
      "    \"period_reasoning\": \"The play was written in 1639, which falls within the Early Modern period.\",\n",
      "    \"timeframe_start\": \"1450-01-01\",\n",
      "    \"timeframe_end\": \"1789-12-31\",\n",
      "    \"location\": \"France\",\n",
      "    \"location_reasoning\": \"The author is Tristan L'Hermite, a French playwright.\",\n",
      "    \"location_qid\": \"Q310\"\n",
      "}\n",
      "```\n",
      "Model: ollama:gemma2:9b; Prompt: Look at the following JSON object describing a theatre play in French (XVII century); the `metadata` property contains basic information about the play (author, title, publication date).\n",
      "\n",
      "INPUT:\n",
      "```json\n",
      "{\n",
      "  \"metadata\": {\n",
      "    \"author\": \"La Fert√©, de\",\n",
      "    \"title\": \"Le Carnaval de Lyon, com√©die\",\n",
      "    \"publication_date\": \"1699\",\n",
      "    \"document_id\": \"bpt6k8569801\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "Your role is to predict the location and historical period in which the action of the play is set. \n",
      "\n",
      "KEY RULES:\n",
      "- Predict the timespan and not the precise and exact date of the period where the play could have taken place\n",
      "- Do not write an introduction or summary \n",
      "- The response must contain only valid JSON\n",
      "- The values in the JSON \"timeframe_start\" and \"timeframe_end\" should always be a single valid ISO date in the form YYYY-MM-DD\n",
      "- if the provided information is not sufficient to determine historical period and/or location, the following values can be set to `None`: `period`, `timeframe_start`, `timeframe_end`, `location`, `location_qid`\n",
      "\n",
      "Return your response and the underlying reasoning as a JSON object with the following structure:\n",
      "```json\n",
      "{\n",
      "    \"period\": \"The historical period in which the play could have taken place\",\n",
      "    \"period_reasoning\": \"The reasoning the model used to identify the historical period\",\n",
      "    \"timeframe_start\": \"The ISO-formatted start value of the historical period\",\n",
      "    \"timeframe_end\": \"The ISO-formatted start value of the historical period\",\n",
      "    \"location\": \"The geographic location where the action of the play takes place\",\n",
      "    \"location_reasoning\": \"The reasoning the model used to identify the geographic location\",\n",
      "    \"location_qid\": \"The Wikidata QID of the identified location\"\n",
      "}\n",
      "```\n",
      "Response: ```json\n",
      "{\n",
      "    \"period\": \"17th century\",\n",
      "    \"period_reasoning\": \"The publication date is 1699, which places it in the 17th century.\",\n",
      "    \"timeframe_start\": \"1600-01-01\",\n",
      "    \"timeframe_end\": \"1700-01-01\",\n",
      "    \"location\": \"Lyon\",\n",
      "    \"location_reasoning\": \"The title of the play is 'Le Carnaval de Lyon, com√©die'.\",\n",
      "    \"location_qid\": \"Q823\" \n",
      "}\n",
      "```\n",
      "Model: ollama:gemma2:9b; Prompt: Look at the following JSON object describing a theatre play in French (XVII century); the `metadata` property contains basic information about the play (author, title, publication date), while the `excerpt` property contains an excerpt of 400 words sampled from around the middle of the document.\n",
      "\n",
      "INPUT:\n",
      "```json\n",
      "{\n",
      "  \"metadata\": {\n",
      "    \"author\": \"La Fert√©, de\",\n",
      "    \"title\": \"Le Carnaval de Lyon, com√©die\",\n",
      "    \"publication_date\": \"1699\",\n",
      "    \"document_id\": \"bpt6k8569801\"\n",
      "  },\n",
      "  \"excerpt\": \"ouyent-ils trop √¢prement tourments de la crainte du mal dont ou ne plaint personne? faites leur prendre par un amant de poids de cet, aurum lenitivum, pr√©par√© √† la pharmacie du grand bureau des finances, Messieurs voulez vous un pr√©servatif su contres dangereuses morsures de ces animaux venide vos magistrats n'y sauraient extirper? frot vous tous les matins de cette pomade anodine. cure recr√©ativu\"\n",
      "}\n",
      "```\n",
      "\n",
      "Your role is to predict the location and historical period in which the action of the play is set. \n",
      "\n",
      "KEY RULES:\n",
      "- Predict the timespan and not the precise and exact date of the period where the play could have taken place\n",
      "- Do not write an introduction or summary \n",
      "- The response must contain only valid JSON\n",
      "- The values in the JSON \"timeframe_start\" and \"timeframe_end\" should always be a single valid ISO date in the form YYYY-MM-DD\n",
      "- if the provided information is not sufficient to determine historical period and/or location, the following values can be set to `None`: `period`, `timeframe_start`, `timeframe_end`, `location`, `location_qid`\n",
      "\n",
      "Return your response and the underlying reasoning as a JSON object with the following structure:\n",
      "```json\n",
      "{\n",
      "    \"period\": \"The historical period in which the play could have taken place\",\n",
      "    \"period_reasoning\": \"The reasoning the model used to identify the historical period\",\n",
      "    \"timeframe_start\": \"The ISO-formatted start value of the historical period\",\n",
      "    \"timeframe_end\": \"The ISO-formatted start value of the historical period\",\n",
      "    \"location\": \"The geographic location where the action of the play takes place\",\n",
      "    \"location_reasoning\": \"The reasoning the model used to identify the geographic location\",\n",
      "    \"location_qid\": \"The Wikidata QID of the identified location\"\n",
      "}\n",
      "```\n",
      "Response: ```json\n",
      "{\n",
      "    \"period\": \"17th century\",\n",
      "    \"period_reasoning\": \"The metadata states that the play was published in 1699, placing it in the 17th century.\",\n",
      "    \"timeframe_start\": \"1601-01-01\",\n",
      "    \"timeframe_end\": \"1700-12-31\",\n",
      "    \"location\": \"Lyon\",\n",
      "    \"location_reasoning\": \"The title of the play is 'Le Carnaval de Lyon, com√©die', clearly indicating that the action takes place in Lyon.\",\n",
      "    \"location_qid\": \"Q476\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "for r in llm_responses:\n",
    "    print(f'Model: {r.model_name}; Prompt: {r.prompt}')\n",
    "    print(f'Response: {r.response}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
