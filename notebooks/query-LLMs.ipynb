{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aisuite as ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ai.Client()\n",
    "client.configure({\n",
    "  \"ollama\" : {\n",
    "    \"timeout\": 600,\n",
    "  }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test `aisuite` with dummy prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Respond in Pirate English. Always try to include the phrase - No rum No fun.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a joke about Captain Jack Sparrow\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME               ID              SIZE      MODIFIED          \n",
      "phi4:latest        ac896e5b8b34    9.1 GB    55 seconds ago       \n",
      "gemma2:9b          ff02c3702f32    5.4 GB    41 minutes ago       \n",
      "llama3.2:latest    a80c4f17acd5    2.0 GB    About an hour ago    \n",
      "deepseek-r1:8b     28f8fd6cdc67    4.9 GB    2 hours ago          \n",
      "llama3.3:latest    a6eb4748fd29    42 GB     3 weeks ago          \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    #\"ollama:deepseek-r1:8b\",\n",
    "    #\"ollama:llama:3.3:latest\",\n",
    "    \"ollama:llama3.2:latest\",\n",
    "    \"ollama:gemma2:9b\",\n",
    "    \"ollama:phi4:latest\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies = {}\n",
    "\n",
    "for selected_model in models:\n",
    "    response = client.chat.completions.create(model=selected_model, messages=messages)\n",
    "    replies[selected_model] = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ollama:llama3.2:latest; reply's length = 243\n",
      "Model: ollama:gemma2:9b; reply's length = 304\n"
     ]
    }
   ],
   "source": [
    "for k,v in replies.items():\n",
    "    print(f\"Model: {k}; reply's length = {len(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ollama:llama3.2:latest': \"Yer lookin' fer a joke about that scurvy dog, eh? Alright then, listen close:\\n\\nWhy did Captain Jack Sparrow bring a ladder aboard his ship?\\n\\nBecause he heard the drinks were on the house! Arrr, no rum, no fun!\",\n",
       " 'ollama:gemma2:9b': \"Ahoy, matey! Ye want a tale 'bout ol' Jack Sparrow? \\n\\nGather 'round and listen close:\\n\\nWhy did Captain Jack Sparrow always carry two compasses? \\n\\nTo be sure he wasn't lost at sea...and to have one to point the way to the nearest grog stash! No rum, no fun, ye hear?  üçªüíÄ\\n\\n\\n\",\n",
       " 'ollama:phi4:latest': 'Ahoy there, matey! Gather \\'round for a tale o\\' ol\\' Cap\\'n Jack Sparrow!\\n\\nSo, what happens when you mix Captain Jack Sparrow with a chicken?\\n\\nYou get... \"Cluckin\\' up the wrong ship!\"\\n\\nArrr, no rum, no fun! But remember, ye never know where yer adventure will take ye next!'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query LLMs with real TextEnt data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for each document, load the pre-generated summary\n",
    "- based on the summary, for each doc generate 3 prompts (metadata, metadata + incipit, metadata + summary)\n",
    "- iterate over doc, iterate over prompts per doc, iterate over models, then query with triples (docu, model, prompt)\n",
    "\n",
    "- start with a spacy document\n",
    "- load the corresponding pre-generated summary\n",
    "- define a `build_prompts` function that takes a `spacy_doc` as input and returns a list of tuples `('prompt-id', 'prompt-message')` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "def build_summary_prompt(spacy_doc: Doc) -> str:\n",
    "    \"\"\"\n",
    "    Builds a summary prompt based on a spaCy document.\n",
    "\n",
    "    Args:\n",
    "        spacy_doc (Doc): A spaCy document object containing the text and metadata.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted summary prompt.\n",
    "\n",
    "    The summary is loaded from a JSON file located in the \"../data/summaries\" directory.\n",
    "    The filename of the summary is derived from the 'document_id' stored in the user_data attribute of the spaCy document.\n",
    "    \"\"\"\n",
    "\n",
    "    # load base prompt\n",
    "    with open(\"../data/prompts/summary_prompt.txt\", \"r\") as file:\n",
    "        base_prompt = file.read()\n",
    "\n",
    "    # load the pre-computed summary from its JSON file\n",
    "    summaries_path = Path(\"../data/summaries\")\n",
    "    doc_summary_path = summaries_path / f\"{spacy_doc.user_data['document_id']}_summary.json\"\n",
    "\n",
    "    try:    \n",
    "        with doc_summary_path.open('r', encoding='utf-8') as file:\n",
    "            summary = json.load(file)\n",
    "    except:\n",
    "        #print(f'No summary for document {spacy_doc.user_data[\"document_id\"]}')\n",
    "        return None\n",
    "\n",
    "    # JSON to pretty string\n",
    "    summary_as_string = json.dumps(summary, indent=2, ensure_ascii=False)\n",
    "    return base_prompt.format(document_summary=summary_as_string)\n",
    "\n",
    "\n",
    "def build_excerpt_prompt(spacy_doc: Doc, excerpt_length: int = 400) -> str:\n",
    "    summaries_path = Path(\"../data/summaries\")\n",
    "    doc_summary_path = summaries_path / f\"{spacy_doc.user_data['document_id']}_summary.json\"\n",
    "\n",
    "    # load excerpt prompt\n",
    "    with open(\"../data/prompts/excerpt_prompt.txt\", \"r\") as file:\n",
    "        prompt = file.read()\n",
    "\n",
    "    # load the pre-computed summary from its JSON file\n",
    "    summaries_path = Path(\"../data/summaries\")\n",
    "    doc_summary_path = summaries_path / f\"{spacy_doc.user_data['document_id']}_summary.json\"\n",
    "    \n",
    "    try:    \n",
    "        with doc_summary_path.open('r', encoding='utf-8') as file:\n",
    "            summary = json.load(file)\n",
    "    except:\n",
    "        #print(f'No summary for document {spacy_doc.user_data[\"document_id\"]}')\n",
    "        return None\n",
    "\n",
    "    text_length = len(spacy_doc.text)\n",
    "    mid_point = text_length // 2\n",
    "    left_boundary = mid_point - (excerpt_length // 2) \n",
    "    right_boundary = mid_point + (excerpt_length // 2)\n",
    "    excerpt = spacy_doc.text[left_boundary:right_boundary]\n",
    "\n",
    "    # JSON to pretty string\n",
    "    json_doc = {\n",
    "        'metadata': summary['metadata'],\n",
    "        'excerpt': excerpt\n",
    "    }\n",
    "    json_doc_as_string = json.dumps(json_doc, indent=2, ensure_ascii=False)\n",
    "    return prompt.format(document=json_doc_as_string, excerpt_length=excerpt_length)\n",
    "\n",
    "def build_metadata_prompt(spacy_doc: Doc) -> str:\n",
    "    summaries_path = Path(\"../data/summaries\")\n",
    "    doc_summary_path = summaries_path / f\"{spacy_doc.user_data['document_id']}_summary.json\"\n",
    "\n",
    "    # load metadata prompt\n",
    "    with open(\"../data/prompts/metadata_prompt.txt\", \"r\") as file:\n",
    "        metadata_prompt = file.read()\n",
    "\n",
    "    # load the pre-computed summary from its JSON file\n",
    "    summaries_path = Path(\"../data/summaries\")\n",
    "    doc_summary_path = summaries_path / f\"{spacy_doc.user_data['document_id']}_summary.json\"\n",
    "    \n",
    "    try:    \n",
    "        with doc_summary_path.open('r', encoding='utf-8') as file:\n",
    "            summary = json.load(file)\n",
    "    except:\n",
    "        #print(f'No summary for document {spacy_doc.user_data[\"document_id\"]}')\n",
    "        return None\n",
    "\n",
    "    # JSON to pretty string\n",
    "    metadata = {'metadata': summary['metadata']}\n",
    "    metadata_as_string = json.dumps(metadata, indent=2, ensure_ascii=False)\n",
    "    return metadata_prompt.format(document_metadata=metadata_as_string)\n",
    "\n",
    "def build_prompts(spacy_doc: Doc) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Builds prompts based on a spaCy document.\n",
    "\n",
    "    Args:\n",
    "        spacy_doc (Doc): A spaCy document object containing the text and metadata.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, str]]: A list of tuples where each tuple contains a prompt ID and its text.    \n",
    "    \"\"\"  \n",
    "    return [\n",
    "        ('prompt-summary', build_summary_prompt(spacy_doc)),\n",
    "        ('prompt-metadata', build_metadata_prompt(spacy_doc)),\n",
    "        ('prompt-excerpt', build_excerpt_prompt(spacy_doc)),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_generate_prompts(spacy_docs: List[Doc], output_path: Path) -> None:\n",
    "\n",
    "    problems = []\n",
    "\n",
    "    for spacy_doc in tqdm(spacy_docs, desc=\"Pre-generating prompts\"):\n",
    "        doc_id = spacy_doc.user_data[\"document_id\"]\n",
    "        prompts = build_prompts(spacy_doc)\n",
    "\n",
    "        # Define the path to the directory\n",
    "        directory_path = output_path / doc_id\n",
    "\n",
    "        # Check if the directory exists\n",
    "        if not directory_path.exists():\n",
    "            directory_path.mkdir(parents=True, exist_ok=True) # Create the directory if it does not exist\n",
    "\n",
    "        for prompt_id, prompt in prompts:\n",
    "            if prompt:\n",
    "                #print(f\"Writing prompt {prompt_id} for document {doc_id}\")\n",
    "                with open(output_path / doc_id / f\"{doc_id}_{prompt_id}.txt\", \"w\") as file:\n",
    "                    file.write(prompt)\n",
    "            else:\n",
    "                problems.append(f'There was a problem with generating prompt {prompt_id} for document {doc_id}')\n",
    "    \n",
    "    print(\"\\n\".join(problems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "from textentlib.utils import load_or_create_corpus, nlp_model_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACY_CORPUS_SERIALIZED_PATH = Path(\"../data/corpus_24022025.spacy\")\n",
    "PRE_GENERATED_PROMPTS_PATH = Path(\"../data/prompts/pregenerated\")    \n",
    "SAMPLE_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded serialize spacy corpus from ../data/corpus_24022025.spacy\n",
      "Number of documents in the corpus: 594\n",
      "Number of entities in the corpus: 287389\n",
      "Number of tokens in the corpus: 12885306\n"
     ]
    }
   ],
   "source": [
    "spacy_corpus = load_or_create_corpus(SPACY_CORPUS_SERIALIZED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = spacy_corpus.get_docs(nlp_model_fr.vocab)\n",
    "docs = list(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - we may want to exclude documents in the validation set\n",
    "# - we may want to exclude documents that are very long (> 150k tokens)\n",
    "sampled_docs = random.sample(docs, SAMPLE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sampled_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look at the following JSON object describing a theatre play in French (XVII century); the `metadata` property contains basic information about the play (author, title, publication date), while the `excerpt` property contains an excerpt of 400 words sampled from around the middle of the document.\n",
      "\n",
      "INPUT:\n",
      "```json\n",
      "{\n",
      "  \"metadata\": {\n",
      "    \"author\": \"Marcassus, Pierre de\",\n",
      "    \"title\": \"Les pescheurs illustres\",\n",
      "    \"publication_date\": \"1648\",\n",
      "    \"document_id\": \"bpt6k5811892h\"\n",
      "  },\n",
      "  \"excerpt\": \"n nouveau danger. √Ä peine ai-je le bien de la pouvoir entendre, Que j'entends quelle crie: √¥ trop fid√®le Alcandre O√π vas-tu donc? h√© quoi? me miens-tu secourir Gagne gagne la rive; et me laisse p√©rir. Quels exc√®s de bont√© derechef te retardent? Je ne m√©rite plus que tes yeux me regardent. Charm√© du faux √©clat d'une langage si doux Contre les flots mutins ie redouble mes cous: Repousse les assaus d\"\n",
      "}\n",
      "```\n",
      "\n",
      "Your role is to predict the location and historical period in which the action of the play is set. \n",
      "\n",
      "KEY RULES:\n",
      "- Predict the timespan and not the precise and exact date of the period where the play could have taken place\n",
      "- Do not write an introduction or summary \n",
      "- The response must contain only valid JSON\n",
      "- The values in the JSON \"timeframe_start\" and \"timeframe_end\" should always be a single valid ISO date in the form YYYY-MM-DD\n",
      "- if the provided information is not sufficient to determine historical period and/or location, the following values can be set to `None`: `period`, `timeframe_start`, `timeframe_end`, `location`, `location_qid`\n",
      "\n",
      "Return your response and the underlying reasoning as a JSON object with the following structure:\n",
      "```json\n",
      "{\n",
      "    \"period\": \"The historical period in which the play could have taken place\",\n",
      "    \"period_reasoning\": \"The reasoning the model used to identify the historical period\",\n",
      "    \"timeframe_start\": \"The ISO-formatted start value of the historical period\",\n",
      "    \"timeframe_end\": \"The ISO-formatted start value of the historical period\",\n",
      "    \"location\": \"The geographic location where the action of the play takes place\",\n",
      "    \"location_reasoning\": \"The reasoning the model used to identify the geographic location\",\n",
      "    \"location_qid\": \"The Wikidata QID of the identified location\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(build_excerpt_prompt(sampled_docs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sampled_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-generating prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:01<00:00, 42.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was a problem with generating prompt prompt-summary for document bpt6k1522463f\n",
      "There was a problem with generating prompt prompt-metadata for document bpt6k1522463f\n",
      "There was a problem with generating prompt prompt-excerpt for document bpt6k1522463f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pre_generate_prompts(sampled_docs, PRE_GENERATED_PROMPTS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To store the LLM responses:\n",
    "- `llm_responses` > `<doc_id>` > `<doc-id_model-id_prompt-id>`\n",
    "    - e.g. `llm_responses/bpt6k1522463f/bpt6k1522463f_deepseek_prompt-metadata.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
