Act as an evaluator. Compare the generated PREDICTION with the REFERENCE answer. Evaluate the generated response based on the assessment rules detailed here below. Return the requested scores and the reasons behind them as a JSON document. 

```

REFERENCE:
```json
{gt_annotation_json}
```

PREDICTION:
```json
{prediction_json}

Your role as an evaluator is to evaluate the prediction based on the following criteria:

ASSESSMENT RULES:
1. Score for period string (`score_period_string`)
- compare the prediction (field `gt_period`) against the reference (field `pred_period`)
- assign 1 point if the period in the prediction is the same as (or equivalent to) the one in the reference
- assign 0.5 points if the period in the prediction is broader or narrower than the one in the reference (e.g. prediction: "Late XVIIth century", reference: "Early modern period")
- assign 0 points if the period in the prediction is different from the one in the reference
- store this score in the field `score_period_string` of the output JSON document

2. Score for period interval (`score_period_interval`)
- compare the prediction (field `gt_timeframe`) against the reference (field `pred_timeframe`)
- assign 1 point if the prediction interval coincides with the reference interval
- assign 0.5 points if the prediction interval and the reference interval partly overlap
- assign 0 points if there is no overlap between the prediction interval and the reference interval
- store this score in the field `score_period_interval` of the output JSON document 

3. Score for location string (`score_location_string`)
- compare the prediction (field `pred_location`) against the reference (field `gt_preferred_location`)
- assign 1 point if the location in the prediction is the same as (or equivalent to) the one in the reference
- assign 0.5 points if the location in the prediction is contained in the field `gt_accepted_locations`
- assign 0 points if the prediction location and the reference location differ, and if the prediction location is not contained in the field `gt_accepted_locations`
- store this score in the field `score_location_string` of the output JSON document 

4. Score for location QID (`score_location_qid`)
- compare the prediction (field `pred_location_qid`) against the reference (field `gt_preferred_location_QID`)
- assign 1 point if if the location QID in the prediction is the same as the one in the reference
- assign 0.5 points if the location in the prediction is contained in the field `gt_acceptable_location_QIDs`
- assign 0 points if the prediction location QID and the reference location QID differ, and if the prediction location QID is not contained in the field 
- store this score in the field `score_location_qid` of the output JSON document 

Return your assessment and the underlying reasoning as a JSON object with the following structure:
```json
{{
    "score_period_string": "The score for the period string; it must be a value between 0-1",
    "score_period_interval": "The score for the period interval; it must be a value between 0-1",
    "score_location_string": "The score for the location string; it must be a value between 0-1",
    "score_location_qid": "The score for the location QID; it must be a value between 0-1",
    "score_reasons": "A brief explanation of the reasoning behind the scores you assigned"
}}
```